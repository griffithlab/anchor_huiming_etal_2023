{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import regex\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "import os\n",
    "import matplotlib.style as style\n",
    "import matplotlib.cm as mplcm\n",
    "import matplotlib.colors as colors\n",
    "import collections\n",
    "import scipy.stats as ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('/Users/h.xia/Desktop/Griffith_Lab/R_shiny_visualization/anchor_analysis/obj/saturation_analysis/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "## function that takes in 10 peptide sequences and outputs all possible mutation combinations\n",
    "amino_acids = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "def all_possible_mutations(peptides):\n",
    "    new_peptides = []\n",
    "    for i in peptides:\n",
    "        #print(i)\n",
    "        for j in range(0, len(i)):\n",
    "            for k in amino_acids:\n",
    "                new_peptide = i[:j]+k+i[j+1:]\n",
    "                #print(new_peptide)\n",
    "                new_peptides.append(new_peptide)\n",
    "        #break\n",
    "    return new_peptides\n",
    "\n",
    "def three_mutation_sequence_match(seq1, seq2):\n",
    "    m = regex.findall(\"(\"+seq1+\"){s<=2}\", seq2, overlapped=True)\n",
    "    return m!=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating FASTA files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_peptide_database = load_obj(\"combined_peptide_database\")\n",
    "## CHOOSING HLA-A*02:01 for Saturation Analysis:\n",
    "## HLA-A*02:01 8 1524\n",
    "## HLA-A*02:01 9 7383\n",
    "## HLA-A*02:01 10 6939\n",
    "## HLA-A*02:01 11 4175\n",
    "#count = 0\n",
    "#for i in combined_peptide_database.keys():\n",
    "#    for j in combined_peptide_database[i].keys():\n",
    "#        if len(combined_peptide_database[i][j]) < 10:\n",
    "#            count += 1\n",
    "            #print(i, j, len(combined_peptide_database[i][j]))\n",
    "#print(count)\n",
    "hla_a_0201_data = combined_peptide_database['HLA-A*02:01']\n",
    "sizes = [1,5,10,20,50,100,200,500,1000]\n",
    "def random_choice_3_mutations(input_data, total):\n",
    "    new_list = input_data.copy()\n",
    "    peptide_list = []\n",
    "    count = 0\n",
    "    while len(peptide_list) < total:\n",
    "        if count >= 2000: \n",
    "            print('Exceeded Maximum tries, length of peptide list is: '+str(len(peptide_list)))\n",
    "            break\n",
    "        peptide = random.choice(new_list)\n",
    "        new_list.remove(peptide)\n",
    "        #print(peptide)\n",
    "        #if peptide in peptide_list:\n",
    "            #print('Skip')\n",
    "            #continue\n",
    "        curr_list = [peptide]\n",
    "        for i in peptide_list:\n",
    "            if three_mutation_sequence_match(i, peptide): \n",
    "                curr_list = []\n",
    "                break\n",
    "        peptide_list += curr_list\n",
    "        count += 1\n",
    "        #print(peptide_list)\n",
    "    return peptide_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_dict = {}\n",
    "for j in sizes:\n",
    "    saturation_dict[j] = {}\n",
    "    for i in hla_a_0201_data.keys():\n",
    "        #print(len(hla_a_0201_data[i]))\n",
    "        saturation_dict[j][i] = random_choice_3_mutations(hla_a_0201_data[i],total=j)\n",
    "        print('Done with: '+str(i)+'_'+str(j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save_obj(saturation_dict, 'random_round_5/saturation_dict')\n",
    "saturation_dict = load_obj('random_round_3/saturation_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in saturation_dict.keys():\n",
    "    for j in saturation_dict[i].keys():\n",
    "        peptide_set = saturation_dict[i][j]\n",
    "        mutated_set = all_possible_mutations(peptide_set)\n",
    "        fasta_file = open(\"saturation_analysis/HLA_A_0201_anchor_\"+str(j)+\"mer_input_\"+str(i)+\".fa\", 'w+')\n",
    "        for m,n in enumerate(mutated_set):\n",
    "            fasta_file.write(\">\"+str(m+1)+'\\n')\n",
    "            fasta_file.write(n+'\\n')\n",
    "        fasta_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### For finding the sequences that are at most one mutation away\n",
    "def one_mutation_sequence_match(seq1, seq2):\n",
    "    m = regex.findall(\"(\"+seq1+\"){s<=1}\", seq2, overlapped=True)\n",
    "    return m!=[]\n",
    "### For finding the sequences that are exact matches to original\n",
    "def exact_mutation_sequence_match(seq1, seq2):\n",
    "    m = regex.findall(\"(\"+seq1+\"){s<=0}\", seq2, overlapped=True)\n",
    "    return m!=[]\n",
    "\n",
    "### For generating the og_epitopes list from mutated sequences:\n",
    "def create_og_epitope_list(input_data, query_mutations):\n",
    "    og_epitope_list = []\n",
    "    for i in input_data:\n",
    "        if i in set(query_mutations['Epitope Seq']):\n",
    "            og_epitope_list.append(i)\n",
    "    og_epitope_set = set(og_epitope_list)\n",
    "    return og_epitope_set\n",
    "\n",
    "def find_median_score(epitope_list, all_epitope_data):\n",
    "    score_list = []\n",
    "    for i in epitope_list:\n",
    "        epitope = all_epitope_data.loc[all_epitope_data['Epitope Seq'] == i]\n",
    "        [score] = list(set(epitope['Median Score']))\n",
    "        score_list.append(score)\n",
    "    if len(score_list) != len(epitope_list):\n",
    "        print('ERROR')\n",
    "        return None\n",
    "    return score_list\n",
    "        \n",
    "\n",
    "### For creating the dictionary matching all original epitopes to their mutation and scores\n",
    "def create_mutation_dictionary(og_epitopes, all_mutated_epitopes, all_mutated_median):\n",
    "    all_mutated_epitopes_new = all_mutated_epitopes.copy()\n",
    "    mutation_dict = {}\n",
    "    for i in og_epitopes:\n",
    "        count = 0\n",
    "        mutation_dict[i] = {}\n",
    "        for k,j in enumerate(all_mutated_epitopes):\n",
    "            if one_mutation_sequence_match(i,j):\n",
    "                median = all_mutated_median[k]\n",
    "                mutation_dict[i][j] = median\n",
    "                all_mutated_epitopes_new = all_mutated_epitopes_new.drop(k)\n",
    "                count += 1\n",
    "    return mutation_dict\n",
    "\n",
    "\n",
    "amino_acids = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "\n",
    "### for calculating sum differences for each individual epitope for each individual position\n",
    "def calculate_sum_diff(og_epitopes, og_scores, all_mutant_dict): \n",
    "    sum_dict = {}\n",
    "    for (i,j) in zip(og_epitopes, og_scores):\n",
    "        mutant_list = all_mutant_dict[i]\n",
    "        sum_dict[i] = []\n",
    "        for n in range(0, len(i)):\n",
    "            sum_ratio_diff = 0\n",
    "            for k in amino_acids:\n",
    "                query_peptide = i[:n]+k+i[n+1:]\n",
    "                score = mutant_list[query_peptide]\n",
    "                sum_ratio_diff += score/j\n",
    "            sum_dict[i].append([n, sum_ratio_diff])\n",
    "    return sum_dict\n",
    "\n",
    "### for calculating the overall sum differences across all 10 original epitopes \n",
    "def overall_pos_sum_diff(sum_diff_results, pos_length):\n",
    "    overall_score = [0]*pos_length\n",
    "    for i in sum_diff_results.keys():\n",
    "        for j in range(len(i)):\n",
    "            (pos, score) = sum_diff_results[i][j]\n",
    "            overall_score[pos] += score\n",
    "    return overall_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis for 8,9,10,11 length mers\n",
    "#### Need to Manually loop through different random rounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization\n",
    "lengths = [8, 9, 10, 11]\n",
    "overall_pos_scores_dict = {}\n",
    "og_epitope = {}\n",
    "round_num = 2\n",
    "combined_peptide_database = load_obj(\"combined_peptide_database\")\n",
    "hla_a_0201_data = combined_peptide_database['HLA-A*02:01']\n",
    "sizes = [1,5,10,20,50,100,200,500,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saturation_dict = load_obj('random_round_'+str(round_num)+'/saturation_dict')\n",
    "for n_mer in lengths:\n",
    "    print(\"Processing length: \"+ str(n_mer))\n",
    "    ## First find the original epitopes and build a dictionary\n",
    "    for i in sizes:\n",
    "        all_epitopes_n_mer = pd.read_table('./saturation_analysis/round_'+str(round_num)+'/output/pvacbind_'+str(n_mer)+'_'+str(i)+'_output/MHC_Class_I/ANCHOR.all_epitopes.tsv', delimiter='\\t')\n",
    "        og_epitope[i] = create_og_epitope_list(hla_a_0201_data[n_mer], all_epitopes_n_mer)\n",
    "        for i in og_epitope.keys():\n",
    "            extra = []\n",
    "            for j in og_epitope[i]:\n",
    "                if j not in saturation_dict[i][n_mer]:\n",
    "                    extra.append(j)\n",
    "            for k in extra:\n",
    "                og_epitope[i].remove(k)\n",
    "        print(len(og_epitope[i]))\n",
    "\n",
    "    ## Then for each size, calculate the corresponding normalized sum of ratios\n",
    "    for i in sizes:\n",
    "        if i in overall_pos_scores_dict:\n",
    "            print(\"Done: \"+ str(i))\n",
    "            continue\n",
    "        else:\n",
    "            all_epitope_data = pd.read_table('./saturation_analysis/round_'+str(round_num)+'/output/pvacbind_'+str(n_mer)+'_'+str(i)+'_output/MHC_Class_I/ANCHOR.all_epitopes.tsv', delimiter='\\t')\n",
    "            mutation_dictionary = create_mutation_dictionary(og_epitope[i], all_epitope_data['Epitope Seq'], all_epitope_data['Median Score'])\n",
    "            sum_diff_pos = calculate_sum_diff(og_epitope[i], find_median_score(og_epitope[i], all_epitope_data), mutation_dictionary)\n",
    "            overall_pos_scores = [j/i for j in overall_pos_sum_diff(sum_diff_pos,n_mer)]\n",
    "            overall_pos_scores_dict[i] = overall_pos_scores\n",
    "        print(\"Done: \"+ str(i))\n",
    "\n",
    "    ### Save the dictionary generated in corresponding folder\n",
    "    save_obj(overall_pos_scores_dict, 'random_round_'+str(round_num)+'/overall_pos_scores_'+str(n_mer)+'_mer_dict')\n",
    "\n",
    "    ### Then calculate the pearson correlation between ground truth dataset (size 1000) & the data generated from each subsample\n",
    "    standard = overall_pos_scores_dict[1000]\n",
    "    overall_pos_corr = {}\n",
    "    for i in sizes:\n",
    "        trend1 = overall_pos_scores_dict[i]\n",
    "        overall_pos_corr[i], p_val = ss.pearsonr(trend1, standard)\n",
    "    save_obj(overall_pos_corr, 'random_round_'+str(round_num)+'/overall_pos_'+str(n_mer)+'_mer_corr')\n",
    "    overall_pos_scores_dict = {}\n",
    "    og_epitope = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialization\n",
    "for round_num in [1,2,3,4,5]:\n",
    "    print(\"Now in round: \"+str(round_num))\n",
    "    lengths = [8, 9, 10, 11]\n",
    "    sizes = [1,5,10,20,50,100,200,500,1000]\n",
    "    overall_pos_scores_dict = {}\n",
    "    og_epitope = {}\n",
    "    combined_peptide_database = load_obj(\"combined_peptide_database\")\n",
    "    hla_a_0201_data = combined_peptide_database['HLA-A*02:01']\n",
    "\n",
    "    saturation_dict = load_obj('random_round_'+str(round_num)+'/saturation_dict')\n",
    "    for n_mer in lengths:\n",
    "        print(\"Processing length: \"+ str(n_mer))\n",
    "        ## First find the original epitopes and build a dictionary\n",
    "        for i in sizes:\n",
    "            all_epitopes_n_mer = pd.read_table('./saturation_analysis/round_'+str(round_num)+'/output/pvacbind_'+str(n_mer)+'_'+str(i)+'_output/MHC_Class_I/ANCHOR.all_epitopes.tsv', delimiter='\\t')\n",
    "            og_epitope[i] = create_og_epitope_list(hla_a_0201_data[n_mer], all_epitopes_n_mer)\n",
    "            for i in og_epitope.keys():\n",
    "                extra = []\n",
    "                for j in og_epitope[i]:\n",
    "                    if j not in saturation_dict[i][n_mer]:\n",
    "                        extra.append(j)\n",
    "                for k in extra:\n",
    "                    og_epitope[i].remove(k)\n",
    "            print(len(og_epitope[i]))\n",
    "\n",
    "        ## Then for each size, calculate the corresponding normalized sum of ratios\n",
    "        for i in sizes:\n",
    "            if i in overall_pos_scores_dict:\n",
    "                print(\"Done: \"+ str(i))\n",
    "                continue\n",
    "            else:\n",
    "                all_epitope_data = pd.read_table('./saturation_analysis/round_'+str(round_num)+'/output/pvacbind_'+str(n_mer)+'_'+str(i)+'_output/MHC_Class_I/ANCHOR.all_epitopes.tsv', delimiter='\\t')\n",
    "                mutation_dictionary = create_mutation_dictionary(og_epitope[i], all_epitope_data['Epitope Seq'], all_epitope_data['Median Score'])\n",
    "                sum_diff_pos = calculate_sum_diff(og_epitope[i], find_median_score(og_epitope[i], all_epitope_data), mutation_dictionary)\n",
    "                overall_pos_scores = [j/i for j in overall_pos_sum_diff(sum_diff_pos,n_mer)]\n",
    "                overall_pos_scores_dict[i] = overall_pos_scores\n",
    "            print(\"Done: \"+ str(i))\n",
    "\n",
    "        ### Save the dictionary generated in corresponding folder\n",
    "        save_obj(overall_pos_scores_dict, 'random_round_'+str(round_num)+'/overall_pos_scores_'+str(n_mer)+'_mer_dict')\n",
    "\n",
    "        ### Then calculate the pearson correlation between ground truth dataset (size 1000) & the data generated from each subsample\n",
    "        standard = overall_pos_scores_dict[1000]\n",
    "        overall_pos_corr = {}\n",
    "        for i in sizes:\n",
    "            trend1 = overall_pos_scores_dict[i]\n",
    "            overall_pos_corr[i], p_val = ss.pearsonr(trend1, standard)\n",
    "        save_obj(overall_pos_corr, 'random_round_'+str(round_num)+'/overall_pos_'+str(n_mer)+'_mer_corr')\n",
    "        overall_pos_scores_dict = {}\n",
    "        og_epitope = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Results from Rounds 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 11\n",
    "fig = plt.figure()\n",
    "ax1 = fig.add_subplot(111)\n",
    "data = []\n",
    "for round_num in [1,2,3,4,5]:\n",
    "    correlation_dict = load_obj('random_round_'+str(round_num)+'/overall_pos_'+str(length)+'_mer_corr')\n",
    "    x = np.arange(len(correlation_dict.keys()))\n",
    "    y = list(correlation_dict.values())\n",
    "    print(x,y)\n",
    "    data.append(y)\n",
    "    ax1.scatter(x,y, label=str(round_num))\n",
    "    ax1.plot(x,y)\n",
    "print(correlation_dict.keys())\n",
    "ax1.xaxis.set_ticklabels([0, 1, 5, 10, 20, 50, 100, 200, 500, 1000])\n",
    "plt.legend(loc='lower right');\n",
    "plt.title('Saturation Analysis using Spearman correlation for '+str(length)+'-mer peptides')\n",
    "plt.xlabel('Subsample Size')\n",
    "plt.ylabel('Spearman Correlation with Subsample 1000')\n",
    "#plt.savefig('Saturation Analysis using Spearman correlation for '+str(length)+'-mer peptides')\n",
    "plt.show()\n",
    "pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
