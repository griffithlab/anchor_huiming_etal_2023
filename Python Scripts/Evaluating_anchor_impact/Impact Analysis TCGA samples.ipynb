{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import upsetplot\n",
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "import pylab as pl\n",
    "import difflib\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open('/Users/h.xia/Desktop/Griffith_Lab/R_shiny_visualization/anchor_analysis/obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "## Plotting the frequency of anchor positions for a certain length\n",
    "def anchor_pos_freq_by_length(filtered_anchor_dict, length):\n",
    "    pos = [i+1 for i in range(length)]\n",
    "    freq = [0 for i in range(length)]\n",
    "    for i in filtered_anchor_dict.keys():\n",
    "        try:\n",
    "            for j in filtered_anchor_dict[i][length]:\n",
    "                freq[j-1] += 1\n",
    "        except:\n",
    "            continue\n",
    "    plt.bar(pos, freq)\n",
    "    plt.xlabel(\"peptide positions\")\n",
    "    plt.ylabel(\"Frequency in dataset\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    return freq\n",
    "\n",
    "### Method for determining anchor position\n",
    "### criteria:\n",
    "### determine contribution of each position to overall percentage\n",
    "### Set cutoff at X =?\n",
    "    \n",
    "def percentage_cutoff(normalized_list, cutoff):\n",
    "    count = 0\n",
    "    anchor_list = []\n",
    "    #print(normalized_list)\n",
    "    indexed_list = sorted([(i,j) for i,j in enumerate(normalized_list)],key=lambda x:(-x[1],x[0]))\n",
    "    #print(indexed_list)\n",
    "    for n,m in indexed_list:\n",
    "        if count > cutoff:\n",
    "            break\n",
    "        else:\n",
    "            count += m\n",
    "            anchor_list.append(n+1)\n",
    "    return anchor_list\n",
    "\n",
    "def anchor_pos_score_by_contribution(anchor_score_dict, percentage):\n",
    "    normalized_dict = {}\n",
    "    filter_contr = {}\n",
    "    for i in anchor_score_dict.keys():\n",
    "        normalized_dict[i] = {}\n",
    "        filter_contr[i] = {}\n",
    "        for j in anchor_score_dict[i].keys():\n",
    "            sum_val = sum(anchor_score_dict[i][j])\n",
    "            normalized_dict[i][j] = (anchor_score_dict[i][j])/sum_val\n",
    "            filter_contr[i][j] = percentage_cutoff(normalized_dict[i][j], percentage)\n",
    "\n",
    "    return normalized_dict, filter_contr\n",
    "\n",
    "def anchor_pos_score_by_contribution_with_base(anchor_score_dict, percentage):\n",
    "    normalized_dict = {}\n",
    "    filter_contr = {}\n",
    "    for i in anchor_score_dict.keys():\n",
    "        normalized_dict[i] = {}\n",
    "        filter_contr[i] = {}\n",
    "        for j in anchor_score_dict[i].keys():\n",
    "            base_score_dict = anchor_score_dict[i][j]-min(anchor_score_dict[i][j])\n",
    "            sum_val = sum(base_score_dict)\n",
    "            normalized_dict[i][j] = (base_score_dict)/sum_val\n",
    "            filter_contr[i][j] = percentage_cutoff(normalized_dict[i][j], percentage)\n",
    "\n",
    "    return normalized_dict, filter_contr\n",
    "\n",
    "### Shared counting functions\n",
    "def anchor_category_determine(condensed_info_row):\n",
    "    if condensed_info_row['Median Fold Change'] > 1:\n",
    "        if condensed_info_row['Mutation at Anchor'] == 1:\n",
    "            if condensed_info_row['Median WT Score'] <= 500:\n",
    "                decision = 'Reject'\n",
    "                group = '3B'\n",
    "            else:\n",
    "                decision = 'Accept'\n",
    "                group = '2'\n",
    "        else:\n",
    "            decision = 'Accept'\n",
    "            group = '1A'\n",
    "    else:\n",
    "        if condensed_info_row['Mutation at Anchor'] == 1:\n",
    "            decision = 'Reject'\n",
    "            group = '3A'\n",
    "        else:\n",
    "            decision = 'Accept'\n",
    "            group = '1B'\n",
    "    return decision, group\n",
    "    \n",
    "def original_decision(condensed_info_row):\n",
    "    if condensed_info_row['Median Fold Change'] > 1 and condensed_info_row['Median MT Score']:\n",
    "        decision = 'Accept'\n",
    "    else:\n",
    "        decision = 'Reject'\n",
    "    return decision\n",
    "\n",
    "\n",
    "def naive_anchor_category_determine(condensed_info_row):\n",
    "    if condensed_info_row['Median Fold Change'] > 1:\n",
    "        if condensed_info_row['Mutation Position'] in [2,condensed_info_row['Peptide Length']]:\n",
    "            if condensed_info_row['Median WT Score'] <= 500:\n",
    "                decision = 'Reject'\n",
    "                group = '3B'\n",
    "            else:\n",
    "                decision = 'Accept'\n",
    "                group = '2'\n",
    "        else:\n",
    "            decision = 'Accept'\n",
    "            group = '1A'\n",
    "    else:\n",
    "        if condensed_info_row['Mutation Position'] in [2,condensed_info_row['Peptide Length']]:\n",
    "            decision = 'Reject'\n",
    "            group = '3A'\n",
    "        else:\n",
    "            decision = 'Accept'\n",
    "            group = '1B'\n",
    "    return decision, group\n",
    "\n",
    "def count_set_occurrences(input_table, filter_list):\n",
    "    table = []\n",
    "    for index, filters in input_table.iterrows():\n",
    "        members = []\n",
    "        for j in filter_list:\n",
    "            decision = filters[j]\n",
    "            if decision == 'Accept':\n",
    "                members.append(j)\n",
    "        table.append(members)\n",
    "    set_table = set(map(tuple, table))\n",
    "    final_count = Counter(map(tuple, table))\n",
    "    output_table = []\n",
    "    counts = []\n",
    "    for i in set_table:\n",
    "        counts.append(final_count[i])\n",
    "        output_table.append(list(i))\n",
    "    return output_table, counts\n",
    "\n",
    "def difference_between_peptides(a,b):\n",
    "    diff_pos = []\n",
    "    if len(a) != len(b):\n",
    "        print(\"INPUT LENGTHS NOT EQUAL\")\n",
    "    for i, (j, k) in enumerate(zip(a,b)):\n",
    "        if j != k :\n",
    "            diff_pos.append(i+1)\n",
    "    return diff_pos\n",
    "\n",
    "def if_anchor_matches_mutation_pos(mutation_type, wt_peptide, mt_peptide, mutation_pos, HLA_allele, anchor_dict):\n",
    "    matched = False\n",
    "    length = len(mt_peptide)\n",
    "    anchor_list = anchor_dict[HLA_allele][length]\n",
    "    if mutation_type == \"missense\" or mutation_type == 'inframe_del': \n",
    "        if pd.isna(wt_peptide):\n",
    "            print('NULL WT Peptide')\n",
    "        else:\n",
    "            diff_pos = difference_between_peptides(wt_peptide, mt_peptide)\n",
    "            #print(\"Different Position: \", diff_pos, mutation_pos, wt_peptide, mt_peptide)\n",
    "            if set(diff_pos).issubset(anchor_list):\n",
    "                matched = True\n",
    "                #print(\"Matched Anchor: \", anchor_list)\n",
    "    elif mutation_type == 'FS' or mutation_type == 'inframe_ins': ### DOES THIS NEED CHANGING? inframe_ins should be followed by difference_between_peptides?\n",
    "        if mutation_pos != 0 or mutation_pos != -1:\n",
    "            diff_pos = list(range(mutation_pos, length+1))\n",
    "            if set(diff_pos).issubset(anchor_list):\n",
    "                matched = True\n",
    "                #print(\"FS/ins mutation matched to anchor: \", anchor_list) \n",
    "                #print(\"FS/ins mutation positions: \", diff_pos, length, anchor_list, HLA_allele)\n",
    "    return matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cutoff at 80% contribution\n",
    "all_anchor_overall_pos_score_dict_r4 = load_obj('all_anchor_overall_pos_score_dict_r4')\n",
    "normalized_anchor_dict_r4, filtered_anchor_dict_r4 = anchor_pos_score_by_contribution(all_anchor_overall_pos_score_dict_r4, 0.8)\n",
    "anchor_pos_freq_by_length(filtered_anchor_dict_r4,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cutoff at 80% contribution wiâ€ h base cutoff\n",
    "normalized_anchor_dict_r4_base, filtered_anchor_dict_r4_base = anchor_pos_score_by_contribution_with_base(all_anchor_overall_pos_score_dict_r4, 0.8)\n",
    "anchor_pos_freq_by_length(filtered_anchor_dict_r4_base,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_anchor_freq = {}\n",
    "n = 9\n",
    "for i in filtered_anchor_dict_r4_m1:\n",
    "    try:\n",
    "        anchors = \"-\".join([str(j) for j in filtered_anchor_dict_r4_m1[i][n]])\n",
    "    except:\n",
    "        continue\n",
    "    try:\n",
    "        hla_anchor_freq[anchors].append(i)\n",
    "    except:\n",
    "        hla_anchor_freq[anchors] = [i]\n",
    "\n",
    "x = [i for i in hla_anchor_freq.keys()]\n",
    "y = [len(i) for i in hla_anchor_freq.values()]\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.bar(x, y)\n",
    "plt.xticks(rotation=90, size=12)\n",
    "plt.show()\n",
    "{pattern:len(hla_anchor_freq[pattern]) for pattern in hla_anchor_freq}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling for balanced HLA alleles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_1000_m2_filter_500b_1 = pd.read_csv('filter_500b_TCGA_1000_m2_pvacseq_analysis_combined.txt', sep='\\t')\n",
    "tcga_1000_m2_filter_500b_2 = pd.read_csv('filter_500b_TCGA_1000_m2_pvacseq_re_analysis_combined.txt', sep='\\t')\n",
    "tcga_1000_m2_filter_500b = tcga_1000_m2_filter_500b_1.append(tcga_1000_m2_filter_500b_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_anchor_data = pd.DataFrame()\n",
    "\n",
    "hla_alleles_for_r4 = np.sort(tcga_1000_m2_filter_500b['HLA Allele'])\n",
    "print(\"Number of HLA alleles included in samples from method 1: \", len(np.unique(hla_alleles_for_r4)))\n",
    "\n",
    "### For each entry, determine if mutation is at anchor position\n",
    "tcga_1000_m2_condensed_info = []\n",
    "for n, i in tcga_1000_m2_filter_500b.iterrows():\n",
    "    if pd.isna(i['Mutation Position']):\n",
    "        print('mutation position is NA', n)\n",
    "        continue\n",
    "    mut_pos = int(i['Mutation Position'])\n",
    "    wt_seq = i['WT Epitope Seq']\n",
    "    mt_seq = i['MT Epitope Seq']\n",
    "    wt_binding = i['Median WT Score']\n",
    "    mt_binding = i['Median MT Score']\n",
    "    hla_allele = i['HLA Allele']\n",
    "    length = i['Peptide Length']\n",
    "    fold_change = i['Median Fold Change']\n",
    "    mutation_type = i['Variant Type']\n",
    "    try:\n",
    "        anchor_pos_list = filtered_anchor_dict_r4[hla_allele][length]\n",
    "    except:\n",
    "        #anchor_pos_list = [-1]\n",
    "        continue\n",
    "    if if_anchor_matches_mutation_pos(mutation_type, wt_seq, mt_seq, mut_pos, hla_allele, filtered_anchor_dict_r4):\n",
    "        at_anchor = 1\n",
    "    else:\n",
    "        at_anchor = 0\n",
    "    tcga_1000_m2_condensed_info.append([mut_pos, mutation_type, wt_seq, mt_seq, wt_binding, mt_binding, hla_allele, length, fold_change, at_anchor, anchor_pos_list])\n",
    "tcga_1000_m2_condensed_info_df = pd.DataFrame.from_records(tcga_1000_m2_condensed_info, columns=['Mutation Position', 'Variant Type', 'WT Sequence', 'MT Sequence','Median WT Score','Median MT Score', 'HLA Allele', 'Peptide Length', 'Median Fold Change', 'Mutation at Anchor', 'Anchor List'])\n",
    "\n",
    "anchor_based_decision = []\n",
    "anchor_based_group = []\n",
    "og_decision = []\n",
    "naive_anchor_decision = []\n",
    "naive_decision_group = []\n",
    "for n, i in tcga_1000_m2_condensed_info_df.iterrows():\n",
    "    anchor_decision, anchor_group = anchor_category_determine(i)\n",
    "    naive_anchor, naive_group = naive_anchor_category_determine(i)\n",
    "    naive_anchor_decision.append(naive_anchor)\n",
    "    naive_decision_group.append(naive_group)\n",
    "    anchor_based_decision.append(anchor_decision)\n",
    "    anchor_based_group.append(anchor_group)\n",
    "    og_decision.append(original_decision(i))\n",
    "\n",
    "agreed_decision_1_3 = []\n",
    "for i,j in zip(og_decision, anchor_based_decision):\n",
    "    if i == j:\n",
    "        agreed_decision_1_3.append('YES')\n",
    "    else:\n",
    "        agreed_decision_1_3.append('NO')\n",
    "        \n",
    "agreed_decision_1_2 = []\n",
    "for i,j in zip(og_decision, naive_anchor_decision):\n",
    "    if i == j:\n",
    "        agreed_decision_1_2.append('YES')\n",
    "    else:\n",
    "        agreed_decision_1_2.append('NO')\n",
    "        \n",
    "agreed_decision_2_3 = []\n",
    "for i,j in zip(naive_anchor_decision, anchor_based_decision):\n",
    "    if i == j:\n",
    "        agreed_decision_2_3.append('YES')\n",
    "    else:\n",
    "        agreed_decision_2_3.append('NO')\n",
    "        \n",
    "tcga_1000_m2_condensed_info_df['Binding < 500 and FC > 1'] = og_decision\n",
    "tcga_1000_m2_condensed_info_df['Naive Anchor Based decision'] = naive_anchor_decision\n",
    "tcga_1000_m2_condensed_info_df['Naive Anchor Decision Group'] = naive_decision_group\n",
    "tcga_1000_m2_condensed_info_df['Anchor Based decision'] = anchor_based_decision\n",
    "tcga_1000_m2_condensed_info_df['Anchor Decision Group'] = anchor_based_group\n",
    "tcga_1000_m2_condensed_info_df['Do basic & naive decisions agree?'] = agreed_decision_1_2\n",
    "tcga_1000_m2_condensed_info_df['Do naive & calculated decisions agree?'] = agreed_decision_2_3\n",
    "tcga_1000_m2_condensed_info_df['Do basic & calculated decisions agree?'] = agreed_decision_1_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcga_1000_m2_condensed_info_df.to_csv('TCGA_1000_anchor_evaluation_3_type_data_results_balanced_m2_corrected.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(og_decision), Counter(naive_anchor_decision), Counter(anchor_based_decision))\n",
    "filters_decision_combined = pd.DataFrame(columns=['Filter A', 'Filter B', 'Filter C'])\n",
    "filters_decision_combined['Filter A'] = og_decision\n",
    "filters_decision_combined['Filter B'] = naive_anchor_decision\n",
    "filters_decision_combined['Filter C'] = anchor_based_decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Counter(og_decision), Counter(naive_anchor_decision), Counter(anchor_based_decision))\n",
    "filters_decision_combined = pd.DataFrame(columns=['Filter A', 'Filter B', 'Filter C'])\n",
    "filters_decision_combined['Filter A'] = og_decision\n",
    "filters_decision_combined['Filter B'] = naive_anchor_decision\n",
    "filters_decision_combined['Filter C'] = anchor_based_decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upset_table, counts = count_set_occurrences(filters_decision_combined, ['Filter A', 'Filter B', 'Filter C'])\n",
    "\n",
    "for i in zip(upset_table, counts):\n",
    "    print(i)\n",
    "upsetplot.plot(upsetplot.from_memberships(upset_table, data=counts), sort_by='cardinality')\n",
    "#plt.savefig('upset_plot_filters_combined_m2_contribution_80_no_base.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of framshift/indel causing mutation anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missense_matched = []\n",
    "fs_matched = []\n",
    "insert_matched = []\n",
    "del_matched = []\n",
    "for n, i in tcga_1000_m1_filter_500b.iterrows():\n",
    "    if pd.isna(i['Mutation Position']):\n",
    "        print('mutation position is NA', n)\n",
    "        continue\n",
    "    mut_pos = int(i['Mutation Position'])\n",
    "    wt_seq = i['WT Epitope Seq']\n",
    "    mt_seq = i['MT Epitope Seq']\n",
    "    wt_binding = i['Median WT Score']\n",
    "    mt_binding = i['Median MT Score']\n",
    "    hla_allele = i['HLA Allele']\n",
    "    length = i['Peptide Length']\n",
    "    fold_change = i['Median Fold Change']\n",
    "    mutation_type = i['Variant Type']\n",
    "    try:\n",
    "        anchor_pos_list = filtered_anchor_dict_r4[hla_allele][length]\n",
    "    except:\n",
    "        #anchor_pos_list = [-1]\n",
    "        continue\n",
    "    if if_anchor_matches_mutation_pos(mutation_type, wt_seq, mt_seq, mut_pos, hla_allele, filtered_anchor_dict_r4):\n",
    "        at_anchor = 1\n",
    "        if i['Variant Type'] == 'inframe_del':\n",
    "            del_matched.append(n)\n",
    "        if i['Variant Type'] == 'missense':\n",
    "            missense_matched.append(n)\n",
    "        if i['Variant Type'] == 'inframe_ins':\n",
    "            insert_matched.append(n)\n",
    "        if i['Variant Type'] == 'FS':\n",
    "            fs_matched.append(n)\n",
    "    else:\n",
    "        at_anchor = 0\n",
    "fs_matched = tcga_1000_m1_filter_500b[tcga_1000_m1_filter_500b['Variant Type'] == 'FS'].loc[fs_matched]\n",
    "missense_matched = tcga_1000_m1_filter_500b[tcga_1000_m1_filter_500b['Variant Type'] == 'missense'].loc[missense_matched]\n",
    "insert_matched = tcga_1000_m1_filter_500b[tcga_1000_m1_filter_500b['Variant Type'] == 'inframe_ins'].loc[insert_matched]\n",
    "del_matched = tcga_1000_m1_filter_500b[tcga_1000_m1_filter_500b['Variant Type'] == 'inframe_del'].loc[del_matched]\n",
    "\n",
    "print(\"Framshift matched: \", fs_matched.drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"Missense matched: \", missense_matched.drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"Inframe del matched: \", del_matched.drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"Inframe ins matched: \", insert_matched.drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(339+11641+41+1)\n",
    "\n",
    "print(tcga_1000_m1_filter_500b.drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"all fs: \", tcga_1000_m1_filter_500b[tcga_1000_m1_filter_500b['Variant Type'] == 'FS'].drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"all missense: \",tcga_1000_m1_filter_500b[tcga_1000_m1_filter_500b['Variant Type'] == 'missense'].drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"all ins: \",tcga_1000_m1_filter_500b[tcga_1000_m1_filter_500b['Variant Type'] == 'inframe_ins'].drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"all del: \",tcga_1000_m1_filter_500b[tcga_1000_m1_filter_500b['Variant Type'] == 'inframe_del'].drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missense_matched = []\n",
    "fs_matched = []\n",
    "insert_matched = []\n",
    "del_matched = []\n",
    "for n, i in tcga_1000_m2_filter_500b.iterrows():\n",
    "    if pd.isna(i['Mutation Position']):\n",
    "        print('mutation position is NA', n)\n",
    "        continue\n",
    "    mut_pos = int(i['Mutation Position'])\n",
    "    wt_seq = i['WT Epitope Seq']\n",
    "    mt_seq = i['MT Epitope Seq']\n",
    "    wt_binding = i['Median WT Score']\n",
    "    mt_binding = i['Median MT Score']\n",
    "    hla_allele = i['HLA Allele']\n",
    "    length = i['Peptide Length']\n",
    "    fold_change = i['Median Fold Change']\n",
    "    mutation_type = i['Variant Type']\n",
    "    try:\n",
    "        anchor_pos_list = filtered_anchor_dict_r4[hla_allele][length]\n",
    "    except:\n",
    "        #anchor_pos_list = [-1]\n",
    "        continue\n",
    "    if if_anchor_matches_mutation_pos(mutation_type, wt_seq, mt_seq, mut_pos, hla_allele, filtered_anchor_dict_r4):\n",
    "        at_anchor = 1\n",
    "        if i['Variant Type'] == 'inframe_del':\n",
    "            del_matched.append(n)\n",
    "        if i['Variant Type'] == 'missense':\n",
    "            missense_matched.append(n)\n",
    "        if i['Variant Type'] == 'inframe_ins':\n",
    "            insert_matched.append(n)\n",
    "        if i['Variant Type'] == 'FS':\n",
    "            fs_matched.append(n)\n",
    "    else:\n",
    "        at_anchor = 0\n",
    "fs_matched = tcga_1000_m2_filter_500b[tcga_1000_m2_filter_500b['Variant Type'] == 'FS'].loc[fs_matched]\n",
    "missense_matched = tcga_1000_m2_filter_500b[tcga_1000_m2_filter_500b['Variant Type'] == 'missense'].loc[missense_matched]\n",
    "insert_matched = tcga_1000_m2_filter_500b[tcga_1000_m2_filter_500b['Variant Type'] == 'inframe_ins'].loc[insert_matched]\n",
    "del_matched = tcga_1000_m2_filter_500b[tcga_1000_m2_filter_500b['Variant Type'] == 'inframe_del'].loc[del_matched]\n",
    "\n",
    "print(\"Framshift matched: \", fs_matched.drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"Missense matched: \", missense_matched.drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"Inframe del matched: \", del_matched.drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"Inframe ins matched: \", insert_matched.drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "\n",
    "print(tcga_1000_m2_filter_500b.drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"all fs: \", tcga_1000_m2_filter_500b[tcga_1000_m2_filter_500b['Variant Type'] == 'FS'].drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"all missense: \",tcga_1000_m2_filter_500b[tcga_1000_m2_filter_500b['Variant Type'] == 'missense'].drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"all ins: \",tcga_1000_m2_filter_500b[tcga_1000_m2_filter_500b['Variant Type'] == 'inframe_ins'].drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)\n",
    "print(\"all del: \",tcga_1000_m2_filter_500b[tcga_1000_m2_filter_500b['Variant Type'] == 'inframe_del'].drop_duplicates(subset=['MT Epitope Seq', 'HLA Allele'], keep='last').shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patient Level analysis of impact of anchor considerations\n",
    "\n",
    "This was done using 100 random TCGA samples that we had full sets of pvactools data for\n",
    "\n",
    "questions to answer:\n",
    "1. Out of the 100 patients, how many patients had their neoantigens affected? On average how many were affected?\n",
    "2. For patients with mutation burden less than 100, how does this impact their pool of peptide candidates?\n",
    "3. Composition in terms of tumor types for our random 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Randomly select samples from TCGA data\n",
    "## First obtain round 1 + round 2 TCGA data list\n",
    "round_1_list = pd.read_csv('/Users/h.xia/Desktop/Griffith_Lab/R_shiny_visualization/anchor_analysis/TCGA_samples_round1_noUCEC.tsv', sep='\\t', header=None)\n",
    "round_2_list = pd.read_csv('/Users/h.xia/Desktop/Griffith_Lab/R_shiny_visualization/anchor_analysis/TCGA_samples_round2_withUCEC.tsv', sep='\\t', header=None)\n",
    "round_1_list = round_1_list.append(round_2_list, ignore_index=True)\n",
    "\n",
    "## Random selection of 100 patients\n",
    "#random_select = round_1_list.sample(n=100)\n",
    "#save_obj(random_select, 'random_selection_anchor_patient_impact_100_sub')\n",
    "random_select = load_obj('random_selection_anchor_patient_impact_100_sub')\n",
    "dataset = []\n",
    "for i,j in random_select.iterrows():\n",
    "    if j[0] in list(round_2_list[0]):\n",
    "        dataset.append(2)\n",
    "    else:\n",
    "        dataset.append(1)\n",
    "random_select['dataset'] = dataset\n",
    "#random_select.to_csv('Random_selection_anchor_patient_impact_100_samples.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Moved relevant files from cluster for 100 random_samples\n",
    "## Now each patient needs to be analyzed for the peptides changed between analysis and how many variants they started with\n",
    "\n",
    "def calculate_change(patient_data, variant_data, patient_unique_data):\n",
    "    variant_data = variant_data.drop_duplicates(subset=['chromosome_name','start','stop','reference', 'variant'])\n",
    "    variant_count = len(variant_data.index)\n",
    "    all_candidate_count = len(patient_data.index)\n",
    "    candidate_count = len(patient_unique_data.index)\n",
    "    ### For each entry, determine if mutation is at anchor position\n",
    "    patient_condensed_info = []\n",
    "    for n, i in patient_unique_data.iterrows():\n",
    "        if pd.isna(i['Mutation Position']):\n",
    "            print('mutation position is NA', n)\n",
    "            continue\n",
    "        mut_pos = int(i['Mutation Position'])\n",
    "        wt_seq = i['WT Epitope Seq']\n",
    "        mt_seq = i['MT Epitope Seq']\n",
    "        wt_binding = i['Median WT Score']\n",
    "        mt_binding = i['Median MT Score']\n",
    "        hla_allele = i['HLA Allele']\n",
    "        length = i['Peptide Length']\n",
    "        fold_change = i['Median Fold Change']\n",
    "        mutation_type = i['Variant Type']\n",
    "        try:\n",
    "            anchor_pos_list = filtered_anchor_dict_r4[hla_allele][length]\n",
    "        except:\n",
    "            print(\"NO INFO for: \"+hla_allele+\" \"+str(length))\n",
    "            #anchor_pos_list = [-1]\n",
    "            continue\n",
    "        if if_anchor_matches_mutation_pos(mutation_type, wt_seq, mt_seq, mut_pos, hla_allele, filtered_anchor_dict_r4):\n",
    "            at_anchor = 1\n",
    "        else:\n",
    "            at_anchor = 0\n",
    "        patient_condensed_info.append([mut_pos, mutation_type, wt_seq, mt_seq, wt_binding, mt_binding, hla_allele, length, fold_change, at_anchor, anchor_pos_list])\n",
    "    patient_condensed_info_df = pd.DataFrame.from_records(patient_condensed_info, columns=['Mutation Position', 'Variant Type', 'WT Sequence', 'MT Sequence','Median WT Score','Median MT Score', 'HLA Allele', 'Peptide Length', 'Median Fold Change', 'Mutation at Anchor', 'Anchor List'])\n",
    "\n",
    "    anchor_based_decision = []\n",
    "    anchor_based_group = []\n",
    "    og_decision = []\n",
    "    naive_anchor_decision = []\n",
    "    naive_decision_group = []\n",
    "    for n, i in patient_condensed_info_df.iterrows():\n",
    "        anchor_decision, anchor_group = anchor_category_determine(i)\n",
    "        naive_anchor, naive_group = naive_anchor_category_determine(i)\n",
    "        naive_anchor_decision.append(naive_anchor)\n",
    "        naive_decision_group.append(naive_group)\n",
    "        anchor_based_decision.append(anchor_decision)\n",
    "        anchor_based_group.append(anchor_group)\n",
    "        og_decision.append(original_decision(i))\n",
    "\n",
    "    agreed_decision_1_3 = []\n",
    "    for i,j in zip(og_decision, anchor_based_decision):\n",
    "        if i == j:\n",
    "            agreed_decision_1_3.append('YES')\n",
    "        else:\n",
    "            agreed_decision_1_3.append('NO')\n",
    "\n",
    "    agreed_decision_1_2 = []\n",
    "    for i,j in zip(og_decision, naive_anchor_decision):\n",
    "        if i == j:\n",
    "            agreed_decision_1_2.append('YES')\n",
    "        else:\n",
    "            agreed_decision_1_2.append('NO')\n",
    "\n",
    "    agreed_decision_2_3 = []\n",
    "    for i,j in zip(naive_anchor_decision, anchor_based_decision):\n",
    "        if i == j:\n",
    "            agreed_decision_2_3.append('YES')\n",
    "        else:\n",
    "            agreed_decision_2_3.append('NO')\n",
    "\n",
    "    counter_og = Counter(og_decision)\n",
    "    counter_naive = Counter(naive_anchor_decision)\n",
    "    counter_anchor = Counter(anchor_based_decision)\n",
    "    patient_condensed_info_df['Do basic & naive decisions agree?'] = agreed_decision_1_2\n",
    "    patient_condensed_info_df['Do naive & calculated decisions agree?'] = agreed_decision_2_3\n",
    "    patient_condensed_info_df['Do basic & calculated decisions agree?'] = agreed_decision_1_3\n",
    "    \n",
    "    percent_change_1_3 = Counter(agreed_decision_1_3)['NO']/(Counter(agreed_decision_1_3)['NO']+Counter(agreed_decision_1_3)['YES'])\n",
    "    percent_change_2_3 = Counter(agreed_decision_2_3)['NO']/(Counter(agreed_decision_2_3)['NO']+Counter(agreed_decision_2_3)['YES'])\n",
    "    percent_change_1_2 = Counter(agreed_decision_1_2)['NO']/(Counter(agreed_decision_1_2)['NO']+Counter(agreed_decision_1_2)['YES'])\n",
    "\n",
    "    return percent_change_1_2, percent_change_2_3, percent_change_1_3, variant_count, candidate_count, all_candidate_count, counter_og, counter_naive, counter_anchor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_samples = load_obj('random_selection_anchor_patient_impact_100_sub')\n",
    "patient_percentage_counts = []\n",
    "for n, i in random_samples.iterrows():\n",
    "    sample = i[0]\n",
    "    tumor_type = i[1]\n",
    "    patient_unique_data = pd.read_csv(\"/Users/h.xia/Desktop/Griffith_Lab/R_shiny_visualization/anchor_analysis/random_100_samples/\"+sample+\"/filter_500b_topscore.tsv\", sep=\"\\t\")\n",
    "    patient_data = pd.read_csv(\"/Users/h.xia/Desktop/Griffith_Lab/R_shiny_visualization/anchor_analysis/random_100_samples/\"+sample+\"/filter_500b.tsv\", sep=\"\\t\")\n",
    "    variant_data = pd.read_csv(\"/Users/h.xia/Desktop/Griffith_Lab/R_shiny_visualization/anchor_analysis/random_100_samples/\"+sample+\"/TUMOR_NEW.tsv\", sep=\"\\t\")\n",
    "    [percent_12, percent_23, percent_13, v_count, c_count, all_c_count, counter_og, counter_naive, counter_anchor] = calculate_change(patient_data, variant_data, patient_unique_data)\n",
    "    patient_percentage_counts.append([sample, tumor_type, v_count, c_count, all_c_count, counter_og, counter_naive, counter_anchor, percent_12, percent_23, percent_13, int(c_count*percent_12), int(c_count*percent_23), int(c_count*percent_13)])\n",
    "patient_percentage_counts_df = pd.DataFrame.from_records(patient_percentage_counts, columns=['TCGA Sample ID','Tumor Type','Number of Variants', 'Number of Neoantigen Candidates (TopScore)', 'Number of Neoantigen Candidates',\n",
    "                                                                                             'Counter_og', 'Counter_Naive', 'Counter_Anchor',\n",
    "                                                                                             '% Difference between filter A and B', '% Difference between filter B and C',\n",
    "                                                                                             '% Difference between filter A and C','# Difference between filter A and B',\n",
    "                                                                                             '# Difference between filter B and C', '# Difference between filter A and C'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_percentage_counts_df.to_csv(\"Patient-Level impact analysis with tcga data.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of variants vs number of neoantigen candidates\n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "ax.set(xscale=\"log\", yscale=\"log\")\n",
    "g = sns.scatterplot(data=patient_percentage_counts_df, x='Number of Variants', y='Number of Neoantigen Candidates (TopScore)', hue='Tumor Type',ax=ax, s=200)\n",
    "g.set(xlabel='Number of unique variants', ylabel='Number of strong binding neoantigen candidates')\n",
    "plt.xlim(0, 100000)\n",
    "plt.ylim(0, 100000)\n",
    "#plt.savefig(\"Patient_level_analysis_100_variants_vs_candidates.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of variants vs number of neoantigen candidates, tumor type catergory bar plot\n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "g = sns.countplot(data=patient_percentage_counts_df, x='Tumor Type')\n",
    "g.set(xlabel='Tumor Type', ylabel='Counts')\n",
    "plt.xticks(rotation=45)\n",
    "#plt.savefig(\"Patient_level_analysis_sample_type_bar_plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filters = ['A and B', 'B and C', 'A and C']\n",
    "choices = ['#', '%']\n",
    "for filter_1 in filters:\n",
    "    for choice in choices:\n",
    "        f, ax = plt.subplots(figsize=(10,10))\n",
    "        g = sns.histplot(patient_percentage_counts_df, x=choice+' Difference between filter '+filter_1)\n",
    "        #g.set_xscale('log')\n",
    "        plt.savefig(\"Patient_level_analysis_\"+choice+\" Difference between filter \"+filter_1+\".pdf\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_percentage_counts_df[patient_percentage_counts_df['# Difference between filter A and B'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_percentage_counts_df[patient_percentage_counts_df['# Difference between filter A and B'] == 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.median(patient_percentage_counts_df['# Difference between filter A and B'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Using less than 20 strong binding neoantigens as filter (after top score applied)\n",
    "info = []\n",
    "for n, i in patient_percentage_counts_df.iterrows():\n",
    "    info.append([i['Tumor Type'], i['Number of Variants'], i['Number of Neoantigen Candidates (TopScore)'], i['Counter_og']['Accept'], i['Counter_Naive']['Accept'], i['Counter_Anchor']['Accept']])\n",
    "info_df = pd.DataFrame.from_records(info, columns=['Tumor Type','Number of Unique Variants', 'Number of Neoantigen Candidates (TopScore)',\n",
    "                                                  'Accepted by Filter A', 'Accepted by Filter B', 'Accepted by Filter C'])\n",
    "low_candidates_20 = info_df[info_df['Number of Neoantigen Candidates (TopScore)'] < 20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increase_from_A_to_BC = 0\n",
    "increase_from_B_to_C = 0\n",
    "for n, i in low_candidates_20.iterrows():\n",
    "    if (i['Accepted by Filter A'] < i['Accepted by Filter B']) or (i['Accepted by Filter A'] < i['Accepted by Filter C']):\n",
    "        increase_from_A_to_BC += 1\n",
    "    if (i['Accepted by Filter B'] < i['Accepted by Filter C']):\n",
    "        increase_from_B_to_C += 1\n",
    "print(len(low_candidates_20), increase_from_A_to_BC, increase_from_B_to_C)\n",
    "print(increase_from_A_to_BC/len(low_candidates_20), increase_from_B_to_C/len(low_candidates_20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Number of variants vs number of neoantigen candidates, tumor type catergory bar plot\n",
    "f, ax = plt.subplots(figsize=(10,10))\n",
    "round_1_list.columns = ['TCGA Sample ID', 'Tumor Type'] \n",
    "g = sns.countplot(data=round_1_list, x='Tumor Type')\n",
    "g.set(xlabel='Tumor Type', ylabel='Patient Sample Counts')\n",
    "plt.xticks(rotation=45)\n",
    "plt.savefig(\"Patient_level_analysis_all_1356_sample_type_bar_plot.pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
