{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.ion()\n",
    "import os\n",
    "import matplotlib.style as style\n",
    "import matplotlib.cm as mplcm\n",
    "import matplotlib.colors as colors\n",
    "import upsetplot\n",
    "from collections import Counter\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def create_peptide_database(hla_alleles, all_epitopes_combined):\n",
    "    peptide_database = {}\n",
    "    for i in hla_alleles:\n",
    "        if i in peptide_database:\n",
    "            return \"HLA_allele input not unique\"\n",
    "        else:\n",
    "            peptides = all_epitopes_combined[all_epitopes_combined['HLA Allele']==i]\n",
    "            peptide_database[i] = {}\n",
    "            for j in peptides['MT Epitope Seq']:\n",
    "                peptide_length = len(j)\n",
    "                if peptide_length in peptide_database[i]:\n",
    "                    peptide_database[i][peptide_length].append(j)\n",
    "                else:\n",
    "                    peptide_database[i][peptide_length] = [j]\n",
    "            for k in peptide_database[i].keys():\n",
    "                peptide_database[i][k] = list(set(peptide_database[i][k]))\n",
    "    return peptide_database\n",
    "def check_data(data, database, insufficient, prefix='', start=1):\n",
    "    data = data.fillna('')\n",
    "    eligible_data = pd.DataFrame(columns=[\"Patient ID\", \"HLA_Alleles\", \"Adding_allele\", \"Reason\"])\n",
    "    for n, i in data.iterrows():\n",
    "        patient_id = i[0]\n",
    "        for j in list(i[start:]):\n",
    "            if j == '':\n",
    "                continue\n",
    "            if j.split('-')[0] != 'HLA' and prefix == '':\n",
    "                prefix = 'HLA-'\n",
    "            if prefix+j not in database.keys():\n",
    "                \n",
    "                eligible_data = eligible_data.append({\n",
    "                     \"Patient ID\": patient_id,\n",
    "                     \"HLA_Alleles\": ','.join(list(i[start:])),\n",
    "                    \"Adding_allele\": j,\n",
    "                    \"Reason\": \"missing\"\n",
    "                          }, ignore_index=True)\n",
    "            if prefix+j in insufficient:\n",
    "                eligible_data = eligible_data.append({\n",
    "                     \"Patient ID\": patient_id,\n",
    "                     \"HLA_Alleles\": ','.join(list(i[start:])),\n",
    "                    \"Adding_allele\": j,\n",
    "                    \"Reason\": \"insufficient\"\n",
    "                          }, ignore_index=True)\n",
    "            else:\n",
    "                #print(n, j)\n",
    "                continue\n",
    "    return(eligible_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Combine Peptide database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MedImmune Samples\n",
    "all_epitopes_004 = pd.read_csv(\"round_1_datasets/H_MT-10109-004/TUMOR.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_005 = pd.read_csv(\"round_1_datasets/H_MT-10109-005/TUMOR.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_007 = pd.read_csv(\"round_1_datasets/H_MT-10109-007/TUMOR.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_008 = pd.read_csv(\"round_1_datasets/H_MT-10109-008/TUMOR.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_009 = pd.read_csv(\"round_1_datasets/H_MT-10109-009/TUMOR.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_010 = pd.read_csv(\"round_1_datasets/H_MT-10109-010/TUMOR.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_011 = pd.read_csv(\"round_1_datasets/H_MT-10109-011/TUMOR.all_epitopes.tsv\", sep='\\t')\n",
    "\n",
    "## UCLA samples\n",
    "all_epitopes_0461 = pd.read_csv(\"round_1_datasets/UCLA_0461/combined_epitopes.txt\", sep='\\t')\n",
    "all_epitopes_0465 = pd.read_csv(\"round_1_datasets/UCLA_0465/combined_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_0476 = pd.read_csv(\"round_1_datasets/UCLA_0476/0476_combined_all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_0482_1 = pd.read_csv(\"round_1_datasets/UCLA_0482_1/combined_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_0482_2 = pd.read_csv(\"round_1_datasets/UCLA_0482_2/TUMOR.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_0530 = pd.read_csv(\"round_1_datasets/UCLA_0530/TUMOR.all_epitopes.tsv\", sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epitopes_0461_filtered = all_epitopes_0461[all_epitopes_0461['Median MT Score']<=500]\n",
    "all_epitopes_0465_filtered = all_epitopes_0465[all_epitopes_0465['Median MT Score']<=500]\n",
    "all_epitopes_0476_filtered = all_epitopes_0476[all_epitopes_0476['Median MT Score']<=500]\n",
    "all_epitopes_0482_1_filtered = all_epitopes_0482_1[all_epitopes_0482_1['Median MT Score']<=500]\n",
    "all_epitopes_0482_2_filtered = all_epitopes_0482_2[all_epitopes_0482_2['Median MT Score']<=500]\n",
    "all_epitopes_0530_filtered = all_epitopes_0530[all_epitopes_0530['Median MT Score']<=500]\n",
    "ucla_samples_filtered_epitopes = all_epitopes_0461_filtered.append([all_epitopes_0465_filtered, all_epitopes_0476_filtered, all_epitopes_0482_1_filtered,\n",
    "                                                                      all_epitopes_0482_2_filtered, all_epitopes_0530_filtered\n",
    "                                                                      ], ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_epitopes_0461_filtered.shape)\n",
    "print(all_epitopes_0476_filtered.shape)\n",
    "print(all_epitopes_0465_filtered.shape)\n",
    "print(all_epitopes_0482_1_filtered.shape)\n",
    "print(all_epitopes_0482_2_filtered.shape)\n",
    "print(all_epitopes_0530_filtered.shape)\n",
    "print(ucla_samples_filtered_epitopes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_epitopes_004_filtered = all_epitopes_004[all_epitopes_004['Median MT Score']<=500]\n",
    "all_epitopes_005_filtered = all_epitopes_005[all_epitopes_005['Median MT Score']<=500]\n",
    "all_epitopes_007_filtered = all_epitopes_007[all_epitopes_007['Median MT Score']<=500]\n",
    "all_epitopes_008_filtered = all_epitopes_008[all_epitopes_008['Median MT Score']<=500]\n",
    "all_epitopes_009_filtered = all_epitopes_009[all_epitopes_009['Median MT Score']<=500]\n",
    "all_epitopes_010_filtered = all_epitopes_010[all_epitopes_010['Median MT Score']<=500]\n",
    "all_epitopes_011_filtered = all_epitopes_011[all_epitopes_011['Median MT Score']<=500]\n",
    "clinical_samples_filtered_epitopes = all_epitopes_004_filtered.append([all_epitopes_005_filtered, all_epitopes_007_filtered, all_epitopes_008_filtered,\n",
    "                                                                      all_epitopes_009_filtered, all_epitopes_010_filtered, all_epitopes_011_filtered\n",
    "                                                                      ],ignore_index=True, sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_epitopes_004_filtered.shape, all_epitopes_005_filtered.shape, all_epitopes_007_filtered.shape)\n",
    "print(all_epitopes_008_filtered.shape, all_epitopes_009_filtered.shape, all_epitopes_010_filtered.shape)\n",
    "print(all_epitopes_011_filtered.shape)\n",
    "print(clinical_samples_filtered_epitopes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_epitopes_TCGA_300 = pd.read_table(\"Inputs/TCGA_300_filter1a_epitopes_combined.txt\", delimiter='\\t')\n",
    "filtered_epitopes_combined_round_1 = filtered_epitopes_TCGA_300.append([clinical_samples_filtered_epitopes,ucla_samples_filtered_epitopes]\n",
    "                                                                      , ignore_index=True, sort=True)\n",
    "unique_hla_alleles_round_1 = filtered_epitopes_combined_round_1['HLA Allele'].unique()\n",
    "print(len(unique_hla_alleles_round_1))\n",
    "combined_peptide_database_round_1 = create_peptide_database(unique_hla_alleles_round_1, filtered_epitopes_combined_round_1)\n",
    "print(filtered_epitopes_combined_round_1.shape, filtered_epitopes_TCGA_300.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total number of HLA-alleles:'+str(len(combined_peptide_database_round_1.keys())))\n",
    "count = 0\n",
    "for i in combined_peptide_database_round_1.keys():\n",
    "    for j in combined_peptide_database_round_1[i]:\n",
    "        if len(combined_peptide_database_round_1[i][j]) < 10:\n",
    "            count += 1\n",
    "print('Insufficient peptide length combos:', count)\n",
    "print('NOTE: these combos are only counting the ones below 10 but will not count anything that is 0, which has no entry')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing potential additional datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lymphoma Data\n",
    "lymphoma_data = pd.read_csv('./additional_datasets/FL_Neoepitope_final_HLA_typing_sorted.tsv', sep='\\t')\n",
    "## GBM data\n",
    "gbm_data_1 = pd.read_csv('./additional_datasets/twck_classI_hlatypes.tsv', sep='\\t|,', header=None, engine='python')\n",
    "gbm_data_2 = pd.read_csv('./additional_datasets/htc_classI_hlatypes.tsv', sep='\\t|,', header=None, engine='python')\n",
    "print(lymphoma_data.shape, gbm_data_1.shape, gbm_data_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insufficient_data_round_1 = []\n",
    "count = 0\n",
    "for i in combined_peptide_database_round_1.keys():\n",
    "    for j in [8,9,10,11]:\n",
    "        try:\n",
    "            length = len(combined_peptide_database_round_1[i][j])\n",
    "        except:\n",
    "            insufficient_data_round_1.append(i)\n",
    "        if length <= 30:\n",
    "            insufficient_data_round_1.append(i)\n",
    "insufficient_data_round_1 = set(insufficient_data_round_1)\n",
    "print('Round 1: HLA alleles that have insuffcient data in any length: ', len(insufficient_data_round_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm_patient_1 = check_data(gbm_data_1, combined_peptide_database_round_1, insufficient_data_round_1)\n",
    "gbm_patient_2 = check_data(gbm_data_2, combined_peptide_database_round_1, insufficient_data_round_1)\n",
    "gbm_patient_ids = set(list(gbm_patient_1['Patient ID'])+list(gbm_patient_2['Patient ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lymphoma_patient = check_data(lymphoma_data, combined_peptide_database_round_1, insufficient_data_round_1, prefix='HLA-')\n",
    "lymphoma_patient_ids = set(list(lymphoma_patient['Patient ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gbm_patient_ids), len(lymphoma_patient_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lymphoma_hla = []\n",
    "patients_analyzed = []\n",
    "\n",
    "for n, i in lymphoma_patient.iterrows():\n",
    "    if i['Patient ID'] in patients_analyzed:\n",
    "        continue\n",
    "    else:\n",
    "        patients_analyzed.append(i['Patient ID'])\n",
    "        hla_list = i['HLA_Alleles'].split(',')\n",
    "        lymphoma_hla.append([i['Patient ID'], ','.join(['HLA-'+j for j in hla_list])])\n",
    "\n",
    "with open(\"lymphoma_58_patients_hla.tsv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerows(lymphoma_hla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TCGA data\n",
    "tcga_data = pd.read_csv('./additional_datasets/hlaTypesAll.tsv', sep='\\t')\n",
    "tcga_patient= check_data(tcga_data, combined_peptide_database_round_1, insufficient_data_round_1, prefix='', start=2)\n",
    "tcga_patient_ids = set(list(tcga_patient['Patient ID']))\n",
    "print(len(tcga_patient_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_allele = []\n",
    "insufficient_allele = []\n",
    "for i in [gbm_patient_1, gbm_patient_2, lymphoma_patient, tcga_patient]:\n",
    "    for j,k in zip(i['Reason'],i['Adding_allele']) :\n",
    "        if j == 'missing':\n",
    "            if k.split('-')[0] != 'HLA':\n",
    "                missing_allele.append('HLA-'+k)\n",
    "            else:\n",
    "                missing_allele.append(k)\n",
    "        elif j == 'insufficient':\n",
    "            if k.split('-')[0] != 'HLA':\n",
    "                insufficient_allele.append('HLA-'+k)\n",
    "            else:\n",
    "                insufficient_allele.append(k)\n",
    "    print('Insufficient Count: '+str(len(set(insufficient_allele))))\n",
    "    print('Missing Count: '+str(len(set(missing_allele))))\n",
    "    #print(set(missing_allele))\n",
    "    \n",
    "print(len(insufficient_data_round_1), len(set(missing_allele)), len(set(insufficient_allele)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Lymphoma & GBM dataset to current database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filtered_epitopes_combined_round_1 = load_obj('filtered_epitopes_combined_round_1')\n",
    "print(filtered_epitopes_combined_round_1.shape)\n",
    "all_epitopes_cody = pd.read_csv(\"additional_datasets/filter_500bind_cody_54_combined.txt\", sep='\\t')\n",
    "all_epitopes_megan = pd.read_csv(\"additional_datasets/filter_500bind_megan_20_combined.txt\", sep='\\t')\n",
    "all_epitopes_FLNA_1 = pd.read_csv(\"additional_datasets/FLNA-01.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_FLNA_2 = pd.read_csv(\"additional_datasets/FLNA-02.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_FLNA_3 = pd.read_csv(\"additional_datasets/FLNA-03.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_FLNA_4 = pd.read_csv(\"additional_datasets/FLNA-04.all_epitopes.tsv\", sep='\\t')\n",
    "all_epitopes_FLNA_1_filtered = all_epitopes_FLNA_1[all_epitopes_FLNA_1['Median MT Score']<=500]\n",
    "all_epitopes_FLNA_2_filtered = all_epitopes_FLNA_2[all_epitopes_FLNA_2['Median MT Score']<=500]\n",
    "all_epitopes_FLNA_3_filtered = all_epitopes_FLNA_3[all_epitopes_FLNA_3['Median MT Score']<=500]\n",
    "all_epitopes_FLNA_4_filtered = all_epitopes_FLNA_4[all_epitopes_FLNA_4['Median MT Score']<=500]\n",
    "filtered_epitopes_combined_round_2 = filtered_epitopes_combined_round_1.append([all_epitopes_cody, all_epitopes_megan,\n",
    "                                                                               all_epitopes_FLNA_1_filtered, all_epitopes_FLNA_2_filtered,\n",
    "                                                                               all_epitopes_FLNA_3_filtered, all_epitopes_FLNA_4_filtered],\n",
    "                                                                              ignore_index=True, sort=True)\n",
    "\n",
    "print(filtered_epitopes_combined_round_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hla_alleles_round_2 = filtered_epitopes_combined_round_2['HLA Allele'].unique()\n",
    "print(len(unique_hla_alleles_round_2))\n",
    "combined_peptide_database_round_2 = create_peptide_database(unique_hla_alleles_round_2, filtered_epitopes_combined_round_2)\n",
    "print('Total number of HLA-alleles:'+str(len(combined_peptide_database_round_2.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in combined_peptide_database_round_2.keys():\n",
    "    for j in combined_peptide_database_round_2[i]:\n",
    "        if len(combined_peptide_database_round_2[i][j]) < 10:\n",
    "            count += 1\n",
    "print(count)\n",
    "insufficient_data_round_2 = []\n",
    "count = 0\n",
    "for i in combined_peptide_database_round_2.keys():\n",
    "    for j in [8,9,10,11]:\n",
    "        try:\n",
    "            length = len(combined_peptide_database_round_2[i][j])\n",
    "        except:\n",
    "            insufficient_data_round_2.append(i)\n",
    "        if length <= 30:\n",
    "            insufficient_data_round_2.append(i)\n",
    "insufficient_data_round_2 = set(insufficient_data_round_2)\n",
    "print(len(insufficient_data_round_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TCGA DATA ROUND 1 NO UCEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TCGA data after around round 2\n",
    "tcga_data = pd.read_csv('./additional_datasets/hlaTypesAll.tsv', sep='\\t')\n",
    "tcga_patient= check_data(tcga_data, combined_peptide_database_round_2, insufficient_data_round_2, prefix='', start=2)\n",
    "tcga_patient_ids = set(list(tcga_patient['Patient ID']))\n",
    "print(len(tcga_patient_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Looking at what we would gain with additional TCGA samples added\n",
    "missing_allele = []\n",
    "insufficient_allele = []\n",
    "for i in [tcga_patient]:\n",
    "    for j,k in zip(i['Reason'],i['Adding_allele']) :\n",
    "        if j == 'missing':\n",
    "            if k.split('-')[0] != 'HLA':\n",
    "                missing_allele.append('HLA-'+k)\n",
    "            else:\n",
    "                missing_allele.append(k)\n",
    "        elif j == 'insufficient':\n",
    "            if k.split('-')[0] != 'HLA':\n",
    "                insufficient_allele.append('HLA-'+k)\n",
    "            else:\n",
    "                insufficient_allele.append(k)\n",
    "    print('Insufficient Count: '+str(len(set(insufficient_allele))))\n",
    "    print('Missing Count: '+str(len(set(missing_allele))))\n",
    "    \n",
    "print(len(insufficient_data_round_2), len(set(missing_allele)), len(set(insufficient_allele)))\n",
    "print(insufficient_data_round_2-set(insufficient_allele))\n",
    "total_hla_alleles_tcga_round_2 = set(missing_allele + insufficient_allele)\n",
    "print(len(total_hla_alleles_tcga_round_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a dictionary specifying which alleles each sample is adding to our combined dataset\n",
    "tcga_round_3_dict = {}\n",
    "for n, i in tcga_patient.iterrows():\n",
    "    patient_id = i['Patient ID']\n",
    "    corr_hla = i['Adding_allele']\n",
    "    reason = i['Reason']\n",
    "    if patient_id not in tcga_round_3_dict:\n",
    "        tcga_round_3_dict[patient_id] = {}\n",
    "        tcga_round_3_dict[patient_id]['total'] = []\n",
    "        tcga_round_3_dict[patient_id]['missing'] = []\n",
    "        tcga_round_3_dict[patient_id]['insufficient'] = []\n",
    "    if reason == 'insufficient':\n",
    "        tcga_round_3_dict[patient_id]['insufficient'].append(corr_hla)\n",
    "        tcga_round_3_dict[patient_id]['total'].append(corr_hla)\n",
    "    elif reason == 'missing': \n",
    "        tcga_round_3_dict[patient_id]['missing'].append(corr_hla)\n",
    "        tcga_round_3_dict[patient_id]['total'].append(corr_hla)\n",
    "## Uniqify in case one patient has the same HLA allele 2 times\n",
    "for i in tcga_round_3_dict.keys():\n",
    "    for j in tcga_round_3_dict[i]:\n",
    "        if len(tcga_round_3_dict[i][j]) != len(set(tcga_round_3_dict[i][j])):\n",
    "            tcga_round_3_dict[i][j] = list(set(tcga_round_3_dict[i][j]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For each patient's missing and insufficient entries, count and sort by the tuple \n",
    "tcga_round_3_counts = pd.DataFrame()\n",
    "for i in tcga_round_3_dict.keys():\n",
    "    for j in tcga_round_3_dict[i]:\n",
    "        if j == 'missing': \n",
    "            missing_count = len(tcga_round_3_dict[i][j])\n",
    "        if j == 'insufficient': \n",
    "            insuff_count = len(tcga_round_3_dict[i][j])\n",
    "        if j == 'total': \n",
    "            total_count = len(tcga_round_3_dict[i][j])\n",
    "        \n",
    "    tcga_round_3_counts = tcga_round_3_counts.append({'Patient ID': i,'Missing Count':missing_count, 'Insuff Count': insuff_count,\n",
    "                                            'Total Count': total_count}, ignore_index=True)\n",
    "    #print(i, missing_count, insuff_count, total_count)\n",
    "tcga_round_3_counts = tcga_round_3_counts.sort_values(by=['Missing Count', 'Insuff Count'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting TCGA sample names\n",
    "kelsy_tcga_data_samples = pd.read_csv('./additional_datasets/tcgasamplesons3.tsv', sep='\\t', header=None)\n",
    "## Loading TCGA MetaData in order to know which directories from AWS to pull from \n",
    "kelsy_tcga_metadata = pd.read_csv('./additional_datasets/TCGAsamples_metadata.tsv',  sep='\\t', header=None)\n",
    "kelsy_tcga_data_reformat = pd.DataFrame()\n",
    "\n",
    "## Creating a newly formated dataframe with both shortened and original tcga sample name and cohort \n",
    "reformat = []\n",
    "original = []\n",
    "full_datatype = []\n",
    "datatype = []\n",
    "cohort = []\n",
    "for n, i in kelsy_tcga_data_samples.iterrows():\n",
    "    type_of_data = kelsy_tcga_metadata.loc[kelsy_tcga_metadata[0] == i[0],][2].values[0]\n",
    "    full_datatype.append(type_of_data)\n",
    "    try:\n",
    "        datatype.append(type_of_data.split(' ')[2])\n",
    "    except:\n",
    "        datatype.append('N/A')\n",
    "    reformat.append('-'.join(i[0].split('-')[:3]))\n",
    "    original.append(i[0])\n",
    "    cohort.append(kelsy_tcga_metadata.loc[kelsy_tcga_metadata[0] == i[0],][1].values[0])\n",
    "kelsy_tcga_data_reformat['reformat'] = reformat\n",
    "kelsy_tcga_data_reformat['original'] = original\n",
    "kelsy_tcga_data_reformat['full_datatype'] = full_datatype\n",
    "kelsy_tcga_data_reformat['datatype'] = datatype\n",
    "kelsy_tcga_data_reformat['cohort'] = cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exclude all normal samples\n",
    "kelsy_tcga_data_reformat_only_tumor = kelsy_tcga_data_reformat[kelsy_tcga_data_reformat['datatype'] != 'Normal']\n",
    "kelsy_tcga_data_reformat.shape, kelsy_tcga_data_reformat_only_tumor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## total HLA alleles that could be covered by round 2 tcga data: total_hla_alleles_tcga_round_2\n",
    "def check_if_necessary(covered, adding, reason, limit1=4, limit2=4):\n",
    "    ## setting limit for insufficient type as 4 extra and missing type as at least 4\n",
    "    hla_output = []\n",
    "    for i in adding:\n",
    "        count = covered.count(i)\n",
    "        if reason == 'missing':\n",
    "            if count < limit2:\n",
    "                hla_output.append(i)\n",
    "        if reason == 'insufficient':\n",
    "            if count < limit1:\n",
    "                hla_output.append(i)\n",
    "    return hla_output\n",
    "\n",
    "covered_hla_alleles = []\n",
    "ids_included = []\n",
    "ids_excluded = []\n",
    "full_ids_included = []\n",
    "full_ids_excluded = []\n",
    "for n, i in tcga_round_3_counts.iterrows():\n",
    "    patient_id = i['Patient ID']\n",
    "    if patient_id not in list(kelsy_tcga_data_reformat_only_tumor['reformat']):\n",
    "        #print(patient_id)\n",
    "        continue\n",
    "    cancer_type = list(kelsy_tcga_data_reformat_only_tumor.loc[kelsy_tcga_data_reformat_only_tumor[\"reformat\"] == patient_id][\"cohort\"])[0]\n",
    "    if cancer_type == 'UCEC':\n",
    "        continue\n",
    "    full_id = kelsy_tcga_data_reformat_only_tumor.loc[kelsy_tcga_data_reformat_only_tumor['reformat'] == patient_id]['original']\n",
    "    hla_alleles = tcga_round_3_dict[patient_id]\n",
    "    total = hla_alleles['total']\n",
    "    insuff = hla_alleles['insufficient']\n",
    "    missing = hla_alleles['missing']\n",
    "    #print(insuff, check_if_necessary(covered_hla_alleles, insuff, 'insufficient'))\n",
    "    #print(missing, check_if_necessary(covered_hla_alleles, missing, 'missing'))\n",
    "    if len(check_if_necessary(covered_hla_alleles, insuff, 'insufficient')) + len(check_if_necessary(covered_hla_alleles, missing, 'missing')) != 0:\n",
    "        covered_hla_alleles = covered_hla_alleles + hla_alleles['total']\n",
    "        full_ids_included.append(list(full_id)[0])\n",
    "        ids_included.append(patient_id)\n",
    "    else:\n",
    "        #print('Skipping Patient: ', patient_id)\n",
    "        ids_excluded.append(patient_id)\n",
    "        full_ids_excluded.append(list(full_id)[0])\n",
    "print(len(ids_included), len(ids_excluded), len(ids_included)+len(ids_excluded), len(tcga_round_3_counts['Patient ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm what I would gain with subset of additional TCGA samples added\n",
    "tcga_subset = tcga_patient[tcga_patient['Patient ID'].isin(ids_included)]\n",
    "missing_allele_tcga = []\n",
    "insufficient_allele_tcga = []\n",
    "for i in [tcga_subset]:\n",
    "    for j,k in zip(i['Reason'],i['Adding_allele']) :\n",
    "        if j == 'missing':\n",
    "            if k.split('-')[0] != 'HLA':\n",
    "                missing_allele_tcga.append('HLA-'+k)\n",
    "            else:\n",
    "                missing_allele_tcga.append(k)\n",
    "        elif j == 'insufficient':\n",
    "            if k.split('-')[0] != 'HLA':\n",
    "                insufficient_allele_tcga.append('HLA-'+k)\n",
    "            else:\n",
    "                insufficient_allele_tcga.append(k)\n",
    "    print('Insufficient Count: '+str(len(set(insufficient_allele_tcga))))\n",
    "    print('Missing Count: '+str(len(set(missing_allele_tcga))))\n",
    "    \n",
    "print(len(insufficient_data_round_2), len(set(missing_allele_tcga)), len(set(insufficient_allele_tcga)))\n",
    "print(insufficient_data_round_2-set(insufficient_allele_tcga))\n",
    "total_hla_alleles_tcga_round_2 = set(missing_allele_tcga + insufficient_allele_tcga)\n",
    "print(len(total_hla_alleles_tcga_round_2))\n",
    "print ('Still missing samples for these alleles:', set(missing_allele)-set(missing_allele_tcga))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ids_and_cohort = kelsy_tcga_data_reformat_only_tumor[kelsy_tcga_data_reformat_only_tumor['original'].isin(full_ids_included)]\n",
    "full_ids_and_cohort.to_csv(\"TCGA_samples_round1_noUCEC.tsv\", sep=\"\\t\", columns=['original', 'cohort'], index=None, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the HLA datafile for tcga round 1 data\n",
    "tcga_round_1_hla = []\n",
    "patients_analyzed = []\n",
    "\n",
    "for n, i in full_ids_and_cohort.iterrows():\n",
    "    patients_analyzed.append(i['reformat'])\n",
    "    #print(i['reformat'])\n",
    "    hla_data = (tcga_data.loc[tcga_data['patientBarcode'] == i['reformat']]).iloc[0]\n",
    "    hla_string = \",\".join((list(hla_data)[2:8]))\n",
    "    #print(hla_string)\n",
    "    tcga_round_1_hla.append([i['original'], hla_string])\n",
    "\n",
    "with open(\"tcga_round_1_483_patients_hla.tsv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerows(tcga_round_1_hla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Round 1 TCGA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_epitopes_combined_round_2.shape)\n",
    "all_epitopes_tcga_r1 = pd.read_csv(\"./filter_b500_tcga_round1_combined.txt\", sep='\\t')\n",
    "\n",
    "filtered_epitopes_combined_round_3 = filtered_epitopes_combined_round_2.append([all_epitopes_tcga_r1], ignore_index=True, sort=True)\n",
    "\n",
    "print(filtered_epitopes_combined_round_3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hla_alleles_round_3 = filtered_epitopes_combined_round_3['HLA Allele'].unique()\n",
    "print(len(unique_hla_alleles_round_3))\n",
    "combined_peptide_database_round_3 = create_peptide_database(unique_hla_alleles_round_3, filtered_epitopes_combined_round_3)\n",
    "print('Total number of HLA-alleles:'+str(len(combined_peptide_database_round_3.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in combined_peptide_database_round_3.keys():\n",
    "    for j in combined_peptide_database_round_3[i]:\n",
    "        if len(combined_peptide_database_round_3[i][j]) < 10:\n",
    "            count += 1\n",
    "print(count)\n",
    "insufficient_data_round_3 = []\n",
    "count = 0\n",
    "for i in combined_peptide_database_round_3.keys():\n",
    "    for j in [8,9,10,11]:\n",
    "        try:\n",
    "            length = len(combined_peptide_database_round_3[i][j])\n",
    "        except:\n",
    "            insufficient_data_round_3.append(i)\n",
    "        if length <= 30:\n",
    "            insufficient_data_round_3.append(i)\n",
    "insufficient_data_round_3 = set(insufficient_data_round_3)\n",
    "print(len(insufficient_data_round_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining samples for Round 2 TCGA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TCGA data after around round 2\n",
    "tcga_data = pd.read_csv('./additional_datasets/hlaTypesAll.tsv', sep='\\t')\n",
    "tcga_patient_r2= check_data(tcga_data, combined_peptide_database_round_3, insufficient_data_round_3, prefix='', start=2)\n",
    "tcga_patient_ids_r2 = set(list(tcga_patient_r2['Patient ID']))\n",
    "print(len(tcga_patient_ids_r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Build a dictionary specifying which alleles each sample is adding to our combined dataset\n",
    "tcga_round_4_dict = {}\n",
    "for n, i in tcga_patient_r2.iterrows():\n",
    "    patient_id = i['Patient ID']\n",
    "    corr_hla = i['Adding_allele']\n",
    "    reason = i['Reason']\n",
    "    if patient_id not in tcga_round_4_dict:\n",
    "        tcga_round_4_dict[patient_id] = {}\n",
    "        tcga_round_4_dict[patient_id]['total'] = []\n",
    "        tcga_round_4_dict[patient_id]['missing'] = []\n",
    "        tcga_round_4_dict[patient_id]['insufficient'] = []\n",
    "    if reason == 'insufficient':\n",
    "        tcga_round_4_dict[patient_id]['insufficient'].append(corr_hla)\n",
    "        tcga_round_4_dict[patient_id]['total'].append(corr_hla)\n",
    "    elif reason == 'missing': \n",
    "        tcga_round_4_dict[patient_id]['missing'].append(corr_hla)\n",
    "        tcga_round_4_dict[patient_id]['total'].append(corr_hla)\n",
    "## Uniqify in case one patient has the same HLA allele 2 times\n",
    "for i in tcga_round_4_dict.keys():\n",
    "    for j in tcga_round_4_dict[i]:\n",
    "        if len(tcga_round_4_dict[i][j]) != len(set(tcga_round_4_dict[i][j])):\n",
    "            tcga_round_4_dict[i][j] = list(set(tcga_round_4_dict[i][j]))\n",
    "\n",
    "## For each patient's missing and insufficient entries, count and sort by the tuple \n",
    "tcga_round_4_counts = pd.DataFrame()\n",
    "for i in tcga_round_4_dict.keys():\n",
    "    for j in tcga_round_4_dict[i]:\n",
    "        if j == 'missing': \n",
    "            missing_count = len(tcga_round_4_dict[i][j])\n",
    "        if j == 'insufficient': \n",
    "            insuff_count = len(tcga_round_4_dict[i][j])\n",
    "        if j == 'total': \n",
    "            total_count = len(tcga_round_4_dict[i][j])\n",
    "        \n",
    "    tcga_round_4_counts = tcga_round_4_counts.append({'Patient ID': i,'Missing Count':missing_count, 'Insuff Count': insuff_count,\n",
    "                                            'Total Count': total_count}, ignore_index=True)\n",
    "    #print(i, missing_count, insuff_count, total_count)\n",
    "tcga_round_4_counts = tcga_round_4_counts.sort_values(by=['Missing Count', 'Insuff Count'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Converting Kelsy's sample names\n",
    "kelsy_tcga_data_samples = pd.read_csv('./additional_datasets/tcgasamplesons3.tsv', sep='\\t', header=None)\n",
    "## Loading TCGA MetaData in order to know which directories from AWS to pull from \n",
    "kelsy_tcga_metadata = pd.read_csv('./additional_datasets/TCGAsamples_metadata.tsv',  sep='\\t', header=None)\n",
    "kelsy_tcga_data_reformat = pd.DataFrame()\n",
    "\n",
    "## Creating a newly formated dataframe with both shortened and original tcga sample name and cohort \n",
    "reformat = []\n",
    "original = []\n",
    "full_datatype = []\n",
    "datatype = []\n",
    "cohort = []\n",
    "for n, i in kelsy_tcga_data_samples.iterrows():\n",
    "    type_of_data = kelsy_tcga_metadata.loc[kelsy_tcga_metadata[0] == i[0],][2].values[0]\n",
    "    full_datatype.append(type_of_data)\n",
    "    try:\n",
    "        datatype.append(type_of_data.split(' ')[2])\n",
    "    except:\n",
    "        datatype.append('N/A')\n",
    "    reformat.append('-'.join(i[0].split('-')[:3]))\n",
    "    original.append(i[0])\n",
    "    cohort.append(kelsy_tcga_metadata.loc[kelsy_tcga_metadata[0] == i[0],][1].values[0])\n",
    "kelsy_tcga_data_reformat['reformat'] = reformat\n",
    "kelsy_tcga_data_reformat['original'] = original\n",
    "kelsy_tcga_data_reformat['full_datatype'] = full_datatype\n",
    "kelsy_tcga_data_reformat['datatype'] = datatype\n",
    "kelsy_tcga_data_reformat['cohort'] = cohort\n",
    "\n",
    "## exclude all normal samples\n",
    "kelsy_tcga_data_reformat_only_tumor = kelsy_tcga_data_reformat[kelsy_tcga_data_reformat['datatype'] != 'Normal']\n",
    "kelsy_tcga_data_reformat.shape, kelsy_tcga_data_reformat_only_tumor.shape\n",
    "\n",
    "## for round 4 overall, round 2 with TCGA data we are upping the limit to 10 (insuff) & 15 (missing) each \n",
    "def check_if_necessary(covered, adding, reason, limit1=20, limit2=20):\n",
    "    hla_output = []\n",
    "    for i in adding:\n",
    "        count = covered.count(i)\n",
    "        if reason == 'missing':\n",
    "            if count < limit2:\n",
    "                hla_output.append(i)\n",
    "        if reason == 'insufficient':\n",
    "            if count < limit1:\n",
    "                hla_output.append(i)\n",
    "    return hla_output\n",
    "\n",
    "covered_hla_alleles = []\n",
    "ids_included = []\n",
    "ids_excluded = []\n",
    "full_ids_included = []\n",
    "full_ids_excluded = []\n",
    "UCEC_ids_to_include = []\n",
    "\n",
    "round_1_samples = pd.read_csv('TCGA_samples_round1_noUCEC.tsv', sep='\\t', header=None)\n",
    "r1_sample_ids = [\"-\".join(i.split('-')[:-1]) for i in list(set(round_1_samples[0]))]\n",
    "\n",
    "for n, i in tcga_round_4_counts.iterrows():\n",
    "    patient_id = i['Patient ID']\n",
    "    if patient_id not in list(kelsy_tcga_data_reformat_only_tumor['reformat']):\n",
    "        #print(patient_id)\n",
    "        continue\n",
    "    if patient_id in r1_sample_ids:\n",
    "        continue\n",
    "    cancer_type = list(kelsy_tcga_data_reformat_only_tumor.loc[kelsy_tcga_data_reformat_only_tumor[\"reformat\"] == patient_id][\"cohort\"])[0]\n",
    "    full_id = kelsy_tcga_data_reformat_only_tumor.loc[kelsy_tcga_data_reformat_only_tumor['reformat'] == patient_id]['original']\n",
    "    hla_alleles = tcga_round_4_dict[patient_id]\n",
    "    total = hla_alleles['total']\n",
    "    insuff = hla_alleles['insufficient']\n",
    "    missing = hla_alleles['missing']\n",
    "    #print(insuff, check_if_necessary(covered_hla_alleles, insuff, 'insufficient'))\n",
    "    #print(missing, check_if_necessary(covered_hla_alleles, missing, 'missing'))\n",
    "    if len(check_if_necessary(covered_hla_alleles, insuff, 'insufficient')) + len(check_if_necessary(covered_hla_alleles, missing, 'missing')) != 0:\n",
    "        covered_hla_alleles = covered_hla_alleles + hla_alleles['total']\n",
    "        full_ids_included.append(list(full_id)[0])\n",
    "        ids_included.append(patient_id)\n",
    "    else:\n",
    "        #print('Skipping Patient: ', patient_id)\n",
    "        ids_excluded.append(patient_id)\n",
    "        full_ids_excluded.append(list(full_id)[0])\n",
    "print(len(ids_included), len(ids_excluded), len(ids_included)+len(ids_excluded), len(tcga_round_4_counts['Patient ID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## confirm what I would gain with subset of additional TCGA samples added\n",
    "tcga_subset_r2 = tcga_patient_r2[tcga_patient_r2['Patient ID'].isin(ids_included)]\n",
    "missing_allele_tcga_r2 = []\n",
    "insufficient_allele_tcga_r2 = []\n",
    "for i in [tcga_subset_r2]:\n",
    "    for j,k in zip(i['Reason'],i['Adding_allele']) :\n",
    "        if j == 'missing':\n",
    "            if k.split('-')[0] != 'HLA':\n",
    "                missing_allele_tcga_r2.append('HLA-'+k)\n",
    "            else:\n",
    "                missing_allele_tcga_r2.append(k)\n",
    "        elif j == 'insufficient':\n",
    "            if k.split('-')[0] != 'HLA':\n",
    "                insufficient_allele_tcga_r2.append('HLA-'+k)\n",
    "            else:\n",
    "                insufficient_allele_tcga_r2.append(k)\n",
    "    print('Insufficient Count: '+str(len(set(insufficient_allele_tcga_r2))))\n",
    "    print('Missing Count: '+str(len(set(missing_allele_tcga_r2))))\n",
    "    \n",
    "print(len(insufficient_data_round_3), len(set(missing_allele_tcga_r2)), len(set(insufficient_allele_tcga_r2)))\n",
    "print(insufficient_data_round_3-set(insufficient_allele_tcga_r2))\n",
    "total_hla_alleles_tcga_round_3 = set(missing_allele_tcga_r2 + insufficient_allele_tcga_r2)\n",
    "print(len(total_hla_alleles_tcga_round_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_ids_and_cohort = kelsy_tcga_data_reformat_only_tumor[kelsy_tcga_data_reformat_only_tumor['original'].isin(full_ids_included)]\n",
    "full_ids_and_cohort.to_csv(\"TCGA_samples_round2_withUCEC.tsv\", sep=\"\\t\", columns=['original', 'cohort'], index=None, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Creating the HLA datafile for tcga round 2 data\n",
    "tcga_round_2_hla = []\n",
    "patients_analyzed = []\n",
    "\n",
    "for n, i in full_ids_and_cohort.iterrows():\n",
    "    patients_analyzed.append(i['reformat'])\n",
    "    #print(i['reformat'])\n",
    "    hla_data = (tcga_data.loc[tcga_data['patientBarcode'] == i['reformat']]).iloc[0]\n",
    "    hla_string = \",\".join((list(hla_data)[2:8]))\n",
    "    #print(hla_string)\n",
    "    tcga_round_2_hla.append([i['original'], hla_string])\n",
    "\n",
    "with open(\"tcga_round_2_873_patients_hla.tsv\", \"w\", newline=\"\") as f:\n",
    "    writer = csv.writer(f, delimiter='\\t')\n",
    "    writer.writerows(tcga_round_2_hla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(full_ids_included) - set(round_1_samples[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding data from round 2 TCGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_epitopes_combined_round_3.shape)\n",
    "all_epitopes_tcga_r2 = pd.read_csv(\"./filter_b500_tcga_round2_combined.txt\", sep='\\t', low_memory=False)\n",
    "\n",
    "filtered_epitopes_combined_round_4 = filtered_epitopes_combined_round_3.append([all_epitopes_tcga_r2], ignore_index=True, sort=True)\n",
    "\n",
    "print(filtered_epitopes_combined_round_4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_hla_alleles_round_4 = filtered_epitopes_combined_round_4['HLA Allele'].unique()\n",
    "print(len(unique_hla_alleles_round_4))\n",
    "combined_peptide_database_round_4 = create_peptide_database(unique_hla_alleles_round_4, filtered_epitopes_combined_round_4)\n",
    "print('Total number of HLA-alleles:'+str(len(combined_peptide_database_round_4.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for i in combined_peptide_database_round_4.keys():\n",
    "    for j in combined_peptide_database_round_4[i]:\n",
    "        if len(combined_peptide_database_round_4[i][j]) < 10:\n",
    "            count += 1\n",
    "print(count)\n",
    "insufficient_data_round_4 = []\n",
    "count = 0\n",
    "for i in combined_peptide_database_round_4.keys():\n",
    "    for j in [8,9,10,11]:\n",
    "        try:\n",
    "            length = len(combined_peptide_database_round_4[i][j])\n",
    "        except:\n",
    "            insufficient_data_round_4.append(i)\n",
    "        if length <= 30:\n",
    "            insufficient_data_round_4.append(i)\n",
    "insufficient_data_round_4 = set(insufficient_data_round_4)\n",
    "print(len(insufficient_data_round_4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(filtered_epitopes_combined_round_1.shape)\n",
    "print(filtered_epitopes_combined_round_2.shape)\n",
    "print(filtered_epitopes_combined_round_3.shape)\n",
    "print(filtered_epitopes_combined_round_4.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing peptide database for bias towards certain algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best MT score count\n",
    "class_I_algorithms = [\"MHCnuggetsI\", \"MHCflurry\", \"NetMHCcons\", \"NetMHCpan\", \"NetMHC\", \"PickPocket\", \"SMM\", \"SMMPMBEC\"]\n",
    "def count_occurrences(combined_epitopes, algorithm_list):\n",
    "    count_dict = {}\n",
    "    for i in algorithm_list:\n",
    "        count_dict[i] = {'Best': 0, 'Below 500': 0, 'Above 500': 0}\n",
    "    for index, epitope in combined_epitopes.iterrows():\n",
    "        best_score = epitope['Best MT Score Method']\n",
    "        count_dict[best_score]['Best'] += 1\n",
    "        for j in algorithm_list:\n",
    "            query_str = j+\" MT Score\"\n",
    "            try:\n",
    "                binding_score = float(epitope[query_str])\n",
    "            except:\n",
    "                print(index, query_str, epitope[query_str])\n",
    "            if binding_score > 500: count_dict[j]['Above 500'] += 1\n",
    "            else: count_dict[j]['Below 500'] += 1\n",
    "    x_axis_algorithms = list(count_dict.keys())\n",
    "    counts_best = [count_dict[i]['Best'] for i in count_dict.keys()]\n",
    "    counts_below = [count_dict[i]['Below 500'] for i in count_dict.keys()]\n",
    "    counts_above = [count_dict[i]['Above 500'] for i in count_dict.keys()]    \n",
    "    return x_axis_algorithms, counts_best, counts_below, counts_above\n",
    "\n",
    "def count_set_occurrences(combined_epitopes, algorithm_list):\n",
    "    table = []\n",
    "    for index, epitope in combined_epitopes.iterrows():\n",
    "        members = []\n",
    "        for j in algorithm_list:\n",
    "            query_str = j+\" MT Score\"\n",
    "            binding_score = epitope[query_str]\n",
    "            if binding_score < 500: members.append(j)\n",
    "        #if members == []:\n",
    "            #print(index, epitope)\n",
    "        table.append(members)\n",
    "    set_table = set(map(tuple, table))\n",
    "    final_count = Counter(map(tuple, table))\n",
    "    output_table = []\n",
    "    counts = []\n",
    "    for i in set_table:\n",
    "        counts.append(final_count[i])\n",
    "        output_table.append(list(i))\n",
    "    return output_table, counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis_algorithms, counts_best, counts_below, counts_above = count_occurrences(filtered_epitopes_combined_round_4, class_I_algorithms)\n",
    "\n",
    "labels = x_axis_algorithms\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.2  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "rects1 = ax.bar(x - width/2, counts_best, width, label='Best Algorithm')\n",
    "rects2 = ax.bar(x + width/2, counts_below, width, label='Below 500nM')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Counts')\n",
    "ax.set_title('Distribution of best predicting binding algorithms across peptide database', fontsize=11)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "def autolabel(rects):\n",
    "    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.annotate('{}'.format(height),\n",
    "                    xy=(rect.get_x() + rect.get_width() / 2, height),\n",
    "                    xytext=(0, 3),  # 3 points vertical offset\n",
    "                    textcoords=\"offset points\",\n",
    "                    ha='center', va='bottom')\n",
    "\n",
    "\n",
    "autolabel(rects1)\n",
    "autolabel(rects2)\n",
    "ax.set_ylim(0, 2000000)\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upset_table, counts = count_set_occurrences(filtered_epitopes_combined_round_4, class_I_algorithms)\n",
    "upsetplot.plot(upsetplot.from_memberships(upset_table, data=counts), sort_by='cardinality')\n",
    "plt.savefig('upset_plot_combined_peptides_updated_r4.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Random Peptide Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def three_mutation_sequence_match(seq1, seq2):\n",
    "    m = regex.findall(\"(\"+seq1+\"){s<=2}\", seq2, overlapped=True)\n",
    "    return m!=[]\n",
    "\n",
    "def random_choice_3_mutations(input_data, total):\n",
    "    new_list = input_data.copy()\n",
    "    peptide_list = []\n",
    "    count = 0\n",
    "    while len(peptide_list) < total:\n",
    "        if count >= 100: \n",
    "            print('Exceeded Maximum tries, length of peptide list is: '+str(len(peptide_list)))\n",
    "            peptide_list += np.random.choices(new_list,total-len(peptide_list), replace=False)\n",
    "            break\n",
    "        peptide = random.choice(new_list)\n",
    "        new_list.remove(peptide)\n",
    "        curr_list = [peptide]\n",
    "        for i in peptide_list:\n",
    "            if three_mutation_sequence_match(i, peptide): \n",
    "                curr_list = []\n",
    "                break\n",
    "        peptide_list += curr_list\n",
    "        count += 1\n",
    "        #print(peptide_list)\n",
    "    return peptide_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "random_peptide_sets = {}\n",
    "for i in combined_peptide_database.keys():\n",
    "    random_peptide_sets[i] = {}\n",
    "    for k in combined_peptide_database[i].keys():\n",
    "        if len(combined_peptide_database[i][k]) > 10:\n",
    "            count += 1 \n",
    "            #print(i, k, len(combined_peptide_database[i][k]))\n",
    "            random_10 = random_choice_3_mutations(combined_peptide_database[i][k],total=10)\n",
    "            random_peptide_sets[i][k] = random_10\n",
    "        else:\n",
    "            random_peptide_sets[i][k] = combined_peptide_database[i][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_10 = 0\n",
    "count_under = 0\n",
    "count_0 = 0\n",
    "for i in random_peptide_sets:\n",
    "    for j in [8,9,10,11]:\n",
    "        try:\n",
    "            length = len(random_peptide_sets[i][j])\n",
    "            if length == 10:\n",
    "                count_10 += 1\n",
    "            else:\n",
    "                count_under += 1\n",
    "        except:\n",
    "            count_0 += 1\n",
    "print(count_10, count_under, count_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_logo(peptide_list, output_dir):\n",
    "    amino_acids = ['A', 'R', 'N', 'D', 'C', 'E', 'Q', 'G', 'H', 'I', 'L', 'K', 'M', 'F', 'P', 'S', 'T', 'W', 'Y', 'V']\n",
    "    counts_mat = lm.alignment_to_matrix(peptide_list)\n",
    "    cm = plt.get_cmap('viridis')\n",
    "    cNorm  = colors.Normalize(vmin=0, vmax=20)\n",
    "    scalarMap = mplcm.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "    logo = lm.Logo(counts_mat)\n",
    "    for i in range(0,len(peptide_list[0])):\n",
    "        for (j,k) in enumerate(amino_acids):\n",
    "            try:\n",
    "                logo.style_single_glyph(i,k, color=scalarMap.to_rgba(j))\n",
    "            except:\n",
    "                continue\n",
    "    logo.fig.savefig(output_dir)\n",
    "    plt.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATING SEQ LOGOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATING SEQLOGOS for all the data we have available \n",
    "for i in combined_peptide_database.keys():\n",
    "    directory = \"./HLA_seq_logos/\"+i\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for j in combined_peptide_database[i].keys():\n",
    "        create_seq_logo(combined_peptide_database[i][j], './HLA_seq_logos/'+i+'/Length_'+str(j))\n",
    "    print(\"Done: \"+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#color_choices = ['salmon', 'darkorgange', 'forestgreen', 'mediumturquoise', 'palevioletred', 'steelblue',\n",
    "#                'chocolate', 'purple', 'cornflowerblue', 'darkslategrey', ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nonpolar Amino Acids\n",
    "\n",
    "Ala: Alanine           Gly: Glycine          Ile: Isoleucine           Leu: Leucine\n",
    "Met: Methionine  Trp: Tryptophan    Phe: Phenylalanine    Pro: Proline\n",
    "Val: Valine\n",
    "\n",
    "### Polar Amino Acids\n",
    "\n",
    "Cys: Cysteine         Ser: Serine           Thr: Threonine\n",
    "Tyr: Tyrosine       Asn: Asparagine Gln: Glutamine\n",
    "\n",
    "### Polar Basic Amino Acids (Positively Charged)\n",
    "\n",
    "His: Histidine      Lys: Lysine           Arg: Arginine\n",
    "\n",
    "#### Polar Acidic Amino Acids (Negatively Charged)\n",
    "\n",
    "Asp: Aspartate   Glu: Glutamate\n",
    "\n",
    "\n",
    "### Amino acid 3-letter abbreviation 1-letter abbreviation \n",
    "Alanine Ala A Arginine Arg R Asparagine Asn N Aspartic acid Asp D Cysteine Cys C Glutamic acid Glu E Glutamine Gln Q Glycine Gly G Histidine His H Isoleucine Ile I Leucine Leu L Lysine Lys K Methionine Met M Phenylalanine Phe F Proline Pro P Serine Ser S Threonine Thr T Tryptophan Trp W Tyrosine Tyr Y Valine Val V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create Seq logos based on hydrophobicity\n",
    "def create_seq_logo_with_grouping(peptide_list, output_dir):\n",
    "    nonpolar_aa = ['A', 'G', 'I', 'L', 'M', 'W','F','P', 'V']\n",
    "    polar_aa = ['C', 'S', 'Q', 'T', 'Y', 'N']\n",
    "    polar_base = ['R', 'H', 'K']\n",
    "    polar_acid = ['D', 'E']\n",
    "    \n",
    "    counts_mat = lm.alignment_to_matrix(peptide_list)\n",
    "    cm = plt.get_cmap('Dark2')\n",
    "    cNorm  = colors.Normalize(vmin=0, vmax=10)\n",
    "    scalarMap = mplcm.ScalarMappable(norm=cNorm, cmap=cm)\n",
    "    logo = lm.Logo(counts_mat)\n",
    "    for i in range(0,len(peptide_list[0])):\n",
    "        for j in nonpolar_aa:\n",
    "            try:\n",
    "                logo.style_single_glyph(i,j, color=scalarMap.to_rgba(1))\n",
    "            except:\n",
    "                continue\n",
    "        for j in polar_aa:\n",
    "            try:\n",
    "                logo.style_single_glyph(i,j, color=scalarMap.to_rgba(2))\n",
    "            except:\n",
    "                continue\n",
    "        for j in polar_base:\n",
    "            try:\n",
    "                logo.style_single_glyph(i,j, color=scalarMap.to_rgba(3))\n",
    "            except:\n",
    "                continue\n",
    "        for j in polar_acid:\n",
    "            try:\n",
    "                logo.style_single_glyph(i,j, color=scalarMap.to_rgba(4))\n",
    "            except:\n",
    "                continue\n",
    "    logo.fig.savefig(output_dir)\n",
    "    plt.close()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in combined_peptide_database.keys():\n",
    "    directory = \"./HLA_Seqlogo_with_grouping/\"+i\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "    for j in combined_peptide_database[i].keys():\n",
    "        create_seq_logo_with_grouping(combined_peptide_database[i][j], './HLA_Seqlogo_with_grouping/'+i+'/Length_'+str(j))\n",
    "    print(\"Done: \"+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
