{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import csv\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import upsetplot\n",
    "import pylab as pl\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from scipy.spatial import distance\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "\n",
    "def save_obj(obj, name ):\n",
    "    with open('obj/'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('obj/' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading overall pos score dataset\n",
    "all_anchor_overall_pos_score_dict_r4 = load_obj('all_anchor_overall_pos_score_dict_r4')\n",
    "combined_peptide_database_round_4 = load_obj('combined_peptide_database_round_4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_count = 0\n",
    "hla_suff = 0 ## criteria here: at least 3 lengths with 10 \n",
    "comb_count_1 = 0\n",
    "comb_count_10 = 0\n",
    "for i in combined_peptide_database_round_4:\n",
    "    hla_count += 1\n",
    "    #hla_list.append(i)\n",
    "    count_suff = 0\n",
    "    for j in combined_peptide_database_round_4[i]:\n",
    "        if len(combined_peptide_database_round_4[i][j]) >= 1:\n",
    "            comb_count_1 += 1\n",
    "        if len(combined_peptide_database_round_4[i][j]) > 9:\n",
    "            comb_count_10 += 1\n",
    "            count_suff += 1\n",
    "    if count_suff >= 3:\n",
    "        hla_suff += 1\n",
    "    \n",
    "print(hla_count, comb_count_1, comb_count_10, hla_suff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Supplemental Figure: multipanel plotting showing distribution of database\n",
    "distribution = {8:[],9:[],10:[],11:[]}\n",
    "counts = {8:0,9:0,10:0,11:0, 'total':0}\n",
    "for i in combined_peptide_database_round_4.keys():\n",
    "    for j in combined_peptide_database_round_4[i]:\n",
    "        distribution[j].append(len(combined_peptide_database_round_4[i][j]))\n",
    "        counts[j] += len(combined_peptide_database_round_4[i][j])\n",
    "        counts[\"total\"] += len(combined_peptide_database_round_4[i][j])\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('font', family='sans-serif')\n",
    "fig, ax = plt.subplots(2, 2, figsize=(7,5))\n",
    "bin_num = 25\n",
    "for i,j,k in [(0,0,8),(0,1,9),(1,0,10),(1,1,11)]:\n",
    "    maximum = np.max(distribution[k])\n",
    "    minimum = np.min(distribution[k])\n",
    "    logbins = np.logspace(np.log10(minimum),np.log10(maximum),bin_num)\n",
    "    \n",
    "    sns.distplot(distribution[k], ax=ax[i,j], kde=False, bins=logbins, rug=True)\n",
    "    ax[i,j].set_xscale('log')\n",
    "    ax[i,j].set_title('Length '+str(k)+\" (N=\"+str(counts[k])+\")\")\n",
    "    ax[i,j].set_xticklabels([0,0,1,10,100,1000,10000])\n",
    "    #ax[i,j].set_xlabel('Number of peptides')\n",
    "fig.text(0.5, 0.01, 'Number of Peptides per HLA Allele', ha='center')\n",
    "fig.text(0.005, 0.5, 'Density', va='center', rotation='vertical')\n",
    "plt.suptitle(\"Distribution of collected peptides for all 328 HLA alleles\", )\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "#plt.savefig(\"Distribution of collected peptides for all 328 HLA alleles.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering and trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First we need to normalize each allele-length result\n",
    "## Normalize by dividing by sum \n",
    "normalized_dict = {8:{}, 9:{}, 10:{}, 11:{}}\n",
    "normalized_data = {8:[], 9:[], 10:[], 11:[]}\n",
    "normalized_data_labels = {8:[], 9:[], 10:[], 11:[]}\n",
    "for hla in all_anchor_overall_pos_score_dict_r4.keys():\n",
    "    for length in all_anchor_overall_pos_score_dict_r4[hla].keys():\n",
    "        normalized_dict[length][hla] = all_anchor_overall_pos_score_dict_r4[hla][length]/sum(all_anchor_overall_pos_score_dict_r4[hla][length])\n",
    "        normalized_data[length].append(normalized_dict[length][hla])\n",
    "        normalized_data_labels[length].append(hla)\n",
    "\n",
    "for i in normalized_data.keys():\n",
    "    normalized_data[i] = np.stack(normalized_data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OVERALL FIGURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## heatmap of raw data\n",
    "g_heatmap = sns.clustermap(pd.DataFrame.from_dict(normalized_dict[9], orient='index'), col_cluster=False, method='average', xticklabels=[1,2,3,4,5,6,7,8,9], yticklabels=False, cmap='YlGnBu_r')\n",
    "ax = g_heatmap.ax_heatmap\n",
    "ax.set_ylabel(\"HLA alleles\")\n",
    "ax.set_xlabel(\"Positions\")\n",
    "#g_heatmap.savefig('Hierarchical_clustering_with_average_linkage_for_raw_positional_data_horizontal_v2.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_plot_generating(n, length, draw_n, method='average', cluster_draw=[1,2,3,4,5,6,7,8,9,10]):\n",
    "    model_n = AgglomerativeClustering(n_clusters=n, linkage=method)\n",
    "    X = [normalized_dict[length][i] for i in normalized_dict[length]]\n",
    "    model_n = model_n.fit(X)\n",
    "    clusters = [[] for i in range(n)]\n",
    "    for i,j in enumerate(model_n.labels_):\n",
    "        for k in range(n):\n",
    "            if j == k:\n",
    "                clusters[k].append((normalized_data_labels[length][i],normalized_data[length][i]))\n",
    "    mean_of_clusters = [[] for i in range(n)]\n",
    "    for i in range(n):\n",
    "        mean_of_clusters[i] = np.mean([k for j,k in clusters[i]], axis=0)\n",
    "    #print(mean_of_clusters)\n",
    "    ## Plotting subplots             \n",
    "    fig = plt.subplots(draw_n,1, figsize=(10,14))\n",
    "    edge = 10\n",
    "    text_size_s = 5\n",
    "    text_size_b = 15\n",
    "    w = 10\n",
    "    v =1\n",
    "    for i in range(n+1):\n",
    "        if i not in cluster_draw:\n",
    "            continue\n",
    "        else:\n",
    "            i=i-1\n",
    "            ax1 = plt.subplot(draw_n,1,v)\n",
    "            v = v+1\n",
    "            count = 1\n",
    "            hla_str = \"HLA alleles in this cluster: \\n\"\n",
    "            for j in range(0, len(clusters[i])):\n",
    "                ax1.plot(range(1,length+1),clusters[i][j][1])\n",
    "                if count != 0 and count % w == 0:\n",
    "                    hla_str += clusters[i][j][0]\n",
    "                    hla_str += \", \"\n",
    "                    hla_str += \"\\n\"\n",
    "                    count += 1\n",
    "                else:\n",
    "                    hla_str += clusters[i][j][0]\n",
    "                    hla_str += \", \"\n",
    "                    count += 1\n",
    "            plt.subplots_adjust(left=0.3, right=0.9, bottom=0.3, top=0.9)\n",
    "            plt.xlabel(\"Normalized Score\")\n",
    "            plt.ylabel(\"Positions\")\n",
    "            ax1.text(edge,1, hla_str, size=text_size_s, ha=\"left\", va=\"top\", wrap=True)\n",
    "            plt.ylim(0,1)\n",
    "            ax1.plot(range(1,length+1), mean_of_clusters[i], 'w--')\n",
    "\n",
    "\n",
    "    plt.xticks(range(1,length+1))\n",
    "    \n",
    "    plt.suptitle(\"Anchor caluclations plotted for \"+str(n)+\" clusters determined \\n using hierarchical clustering (linkage method \"+method+\") for length \"+str(length), size=text_size_b, ha='center')\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    plt.savefig(\"/Users/h.xia/Desktop/Griffith_Lab/Anchor Paper/Main Figure 2/\"+str(length)+\"mer/Anchor_clusters-length\"+str(length)+\"-\"+str(n)+\"-\"+method+\"clusters_with_mean.pdf\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return clusters, model_n.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_test, labels = cluster_plot_generating(8, 9, 3, cluster_draw=[4,6,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hla_alleles = []\n",
    "for i in normalized_dict:\n",
    "    for j in normalized_dict[i]:\n",
    "        hla_alleles.append(j)\n",
    "len(set(hla_alleles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_mer_data = {}\n",
    "for i in all_anchor_overall_pos_score_dict_r4:\n",
    "    try:\n",
    "        nine_mer_data[i] = all_anchor_overall_pos_score_dict_r4[i][9]\n",
    "    except:\n",
    "        print(\"no 9mer:\" + i)\n",
    "        continue\n",
    "nine_mer_df = pd.DataFrame.from_dict(nine_mer_data, orient='index')\n",
    "nine_mer_norm_data = pd.DataFrame.from_dict(normalized_dict[9], orient='index')\n",
    "method = \"average\"\n",
    "\n",
    "def clusters_calc(n):\n",
    "    model_n = AgglomerativeClustering(n_clusters=n, linkage=method)\n",
    "    X = [normalized_dict[9][i] for i in normalized_dict[9]]\n",
    "    model_n = model_n.fit(X)\n",
    "    clusters = [[] for i in range(n)]\n",
    "    for i,j in enumerate(model_n.labels_):\n",
    "        for k in range(n):\n",
    "            if j == k:\n",
    "                clusters[k].append((normalized_data_labels[9][i],normalized_data[9][i]))\n",
    "    return clusters\n",
    "\n",
    "## orange \n",
    "clusters_3 = clusters_calc(3)\n",
    "orange_hlas = [i for i, j in clusters_3[1]]\n",
    "## green\n",
    "green_hlas = [i for i,j in clusters_3[2]]\n",
    "\n",
    "## red\n",
    "clusters_8 = clusters_calc(8)\n",
    "red_hlas = [i for i,j in clusters_8[3]]\n",
    "\n",
    "## purple\n",
    "purple_hlas = [i for i,j in clusters_8[5]]\n",
    "\n",
    "## blue\n",
    "blue_hlas = [i for i,j in clusters_8[6]]\n",
    "\n",
    "## not colored\n",
    "no_color_hlas = [i for i, j in clusters_8[7]]\n",
    "\n",
    "## overall_color_dict \n",
    "cluster_color_dict = {}\n",
    "def add_cluster(color, hla_list):\n",
    "    for i in hla_list:\n",
    "        cluster_color_dict[i] = color\n",
    "    return None\n",
    "add_cluster(\"orange\", orange_hlas)\n",
    "add_cluster(\"green\", green_hlas)\n",
    "add_cluster(\"red\", red_hlas)\n",
    "add_cluster(\"purple\", purple_hlas)\n",
    "add_cluster(\"blue\", blue_hlas)\n",
    "add_cluster(\"Not colored\", no_color_hlas)\n",
    "\n",
    "print(len(orange_hlas),len(green_hlas),len(red_hlas),len(purple_hlas),len(blue_hlas), len(no_color_hlas))\n",
    "len(orange_hlas) + len(green_hlas) + len(red_hlas) + len(purple_hlas) + len(blue_hlas) + len(no_color_hlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cluster_color_code = []\n",
    "for i,j in nine_mer_norm_data.iterrows():\n",
    "    Cluster_color_code.append(cluster_color_dict[i])\n",
    "nine_mer_norm_data['Cluster Color Codes'] = Cluster_color_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nine_mer_norm_data.to_csv(\"Normalized_anchor_predictions_9_mer_with_color_coding.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_mer_norm_data = pd.DataFrame.from_dict(normalized_dict[8], orient='index')\n",
    "ten_mer_norm_data = pd.DataFrame.from_dict(normalized_dict[10], orient='index')\n",
    "eleven_mer_norm_data = pd.DataFrame.from_dict(normalized_dict[11], orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eight_mer_norm_data.to_csv(\"Normalized_anchor_predictions_8_mer.tsv\", sep=\"\\t\")\n",
    "ten_mer_norm_data.to_csv(\"Normalized_anchor_predictions_10_mer.tsv\", sep=\"\\t\")\n",
    "eleven_mer_norm_data.to_csv(\"Normalized_anchor_predictions_11_mer.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
